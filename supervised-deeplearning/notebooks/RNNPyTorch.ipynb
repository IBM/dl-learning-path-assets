{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <strong>Recurrent Neural Networks Using Pytorch</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook you will learn the basics of a Recurrent Neural Network using the python library Pytorch.\n",
    "---\n",
    "### <strong>Table of Contents</strong>\n",
    "1. [Introduction to Recurrent Neural Networks](#intro)\n",
    "2. [Long-Short Term Memory](#LSTM)\n",
    "3. [Time Series Data](#time)\n",
    "4. [Understanding the Dataset](#data)\n",
    "5. [Using Pytorch](#pytorch)\n",
    "6. [Code](#code)\n",
    "---\n",
    "### By the end of this notebook, you should be able to implement a basic RNN using Pytorch with the provided data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"intro\"></a> <strong>Introduction</strong>\n",
    "---\n",
    "*<strong>Recurrent neural networks</strong>*, or RNNs, are widely used in a variety of mediums. RNNs leverage sequential data to make predictions. **Sequential memory** makes it easier for the neural network to recognize patterns and replicate the input. In order to achieve learning through sequential memory, a **feedforward neural network** with looping mechanisms is implemented. \n",
    "\n",
    "As the image below outlines, there are *three* layers: **input, hidden and output**. There are loops that pass previous information forward, allowing the model to *sequentially* store and learn the data. The complexity of a hidden state is based on how much “historic” information is being stored, it is a representation of all previous steps. When training a model, once there is a prediction from a given output, a **loss function** is used to determine the error between the predicted output and real output. The model is trained through back propagation. The weight of each node in the neural network is adjusted with their corresponding gradient that is calculated during **back propagation**. \n",
    "<br>\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/rnnImg.png\">\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The advantage of using sequential data to successfully predict certain outcomes is especially relevant when analyzing **time series data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"LSTM\"></a><strong>Long-Short Term Memory</strong>\n",
    "---\n",
    "[\"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow\".*](https://ieeexplore.ieee.org/abstract/document/6795963) \n",
    "\n",
    "To solve this issue a type of RNN called **long short term memory** is used. [**LSTMs**](https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-1/) are able to keep track of long term dependencies by using gradients to modify the model, rather than backpropogations. These attributes are especially advantageous when examining **time series data**. The more historical data there is, the better the model can train, producing the most accurate outputs. \n",
    "\n",
    "An LSTM has an internal state variable that is modified based on weights and biases through operation gates. Traditionally, an LSTM is comprised of three operation gates: the forget gate, input gate, and output gate. The architecture of long short term memory is dependent on $tanh$ and $sigmoid$ functions implemented in the network. The $tahn$ function ensures that the values in the network remain between -1 and 1 while the $sigmoid$ function regulates if data should be remembered or forgotten. Furthermore, \n",
    "\n",
    "The mathematical representations of each gate are as follows:\n",
    "\n",
    "<strong>Forget Gate</strong>: $$f_t = \\sigma(w_f*[h_{t-1},x_t] + b_f)$$\n",
    "\n",
    "<strong>Input Gate</strong>: $$i_t = \\sigma(w_f*[h_{t-1},x_t] + b_i)$$\n",
    "\n",
    "<strong>Output Gate</strong>: $$O_t = \\sigma(w_f*[h_{t-1},x_t] + b_o)$$\n",
    "\n",
    "Where:  \n",
    "* $w_f$ = weight matrix between forget and input gate\n",
    "* $h_{t-1}$ = previous hidden state\n",
    "* $x_t$ = input\n",
    "* $b_f$ = connection bias at forget gate \n",
    "* $b_i$ = connection bias at input gate \n",
    "* $b_o$ = connection bias at output gate \n",
    "\n",
    "\n",
    "Each gate modifies the input a different way. The forget gate determines what data is relevant to keep and what information can be \"forgotten\". The input gate analyzes what information needs to be added to the current step, and the output gate finalizes the proceeding hidden state. Each of these gates allows for sequential data to be efficiently stored and analyzed, allowing for an accurate predictive model to be developed. \n",
    "\n",
    "***Source:** *S. Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" in Neural Computation, vol. 9, no. 8, pp. 1735-1780, 15 Nov. 1997, doi: 10.1162/neco.1997.9.8.1735.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"time\"></a><strong>Time Series Data</strong>\n",
    "---\n",
    "Prior to training a model, it is important to understand the type of data you are working with. There are many different types of data, this notebook incorporates time series data. In essence, time series data is a collection of chronologically collected observations made over a period of time- sometimes during specific intervals. Time series data can be grouped as either *<strong>metrics</strong>* or *<strong>events</strong>*. \n",
    "\n",
    "* **Metrics**: measurements taken at regular intervals.\n",
    "\n",
    "* **Events**: measurements taken at irregular intervals. \n",
    "\n",
    "Distinguishing if the data is comprised of metrics or events is critical. Events are not condusive for creating predictive models. The irregular intervals between each data point prevents sequential logic from creating patterns on past behavior. In contrast, the characteristic regularity between each metric allows machine learing models to learn from previous data and construct possible outcomes for the future. Creating an RNN using time series data, specifically metrics, is a great way to take advantage of the sequential learning pattern they leverage. \n",
    "\n",
    "Furthermore, time series data can also be categorized as *<strong>linear</strong>* or *<strong>non-linear</strong>*. Based on the mathematical relationship created by the model, the data is classified as one or the other.\n",
    "\n",
    "Popular examples of time series data include weather, stock, and health care data. In this notebook, we will be using stock data to create an RNN model to predict the value of the given stock. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"data\"></a><strong>Understanding the Dataset</strong>\n",
    "---\n",
    "\n",
    "The data set we are examining has been collected by IBM Watson. It is comprised of the Open, High, Low and Close values of a particular stock from the years 1980-2018. Later in the tutorial, we will extract information about the data's characteristics using certain Pandas methods, however, let us focus on identifying what we want to do with the data first. \n",
    "\n",
    "When describing a stocks value on any given day, the close value is used. Financial institutions also examine close values to analyze a stock's market performance as it represents how the stock performed through market hours. The overall importance of the Close value indicates to us that this is the value that we should train the model to predict. \n",
    "\n",
    "In this tutorial, we will be using all four data fields to create a multivariate model that can precit the Close Value for this stock. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"pytorch\"></a><strong>Using Pytorch</strong>\n",
    "---\n",
    "Pytorch is a python library that uses the specialized data structure Tensors to encode model parameters and inputs. The following is a brief tutorial on imports that we will be using to evaluate the stock data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Pytorch, we must first import the library into our workspace. To do this type the following code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to work with data and create a neural network, we can use the Pytorch class nn and the primatives DataLoader and datasets. Dataset is meant to wrap an iterable around the dataset while DataLoader is meant to load and store the desired data. The matplotlib import allows us to change, create and plot a figure in a plotting area. This is useful for the model we are trying to create in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what imports to use, we are ready to begin creating our model for our stock data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <a name=\"code\"></a><strong>Code</strong>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install pandas\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following empty code cell, import the Stock Data CSV file to your notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert stock data import here, follow the directions outlined in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us examine the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = df_data_1\n",
    "print(\"Information about the dataset\", end = \"\\n\")\n",
    "print(stock_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a total of 6 columns, the columns we want to input into the model are the last four columns: index 2-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five elements in the dataset\", end = \"\\n\")\n",
    "print(stock_data.head(5))\n",
    "print(\"Last five elements in the dataset\", end = \"\\n\")\n",
    "print(stock_data.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the first and last 5 elements, we can see that the data is in inverse order. The most recent data stored at the beginning of the set and the oldest data is at the end. For our model, we need to input the data from oldest to most recent. The Pandas method \"sort_values\" can be used to sort the data in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = stock_data.sort_values(by=\"Date\")\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the \"sort_values\" method, we can use the \"head\" method to print the first five data points. Our data is now in the correct order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = stock_data[['High','Low','Open','Close']]\n",
    "print(price[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the multivariate model we can create an array containing only the variables we want to input. We no longer need the date or index column as the data has already been ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price = scaler.fit_transform(price.values)\n",
    "print(price[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have isolated the target variables, the next step is to normalize the values. This means, representing them as a value between -1 and 1. This allows us to make the data uniform. This means, irrespective of the range it falls under, each data point will have the same impact on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have defined our train window as 7 to represent the length of a week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = 7 \n",
    "import numpy as np\n",
    "def create_inout_sequences(input, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input)\n",
    "    print('Length = ',L)\n",
    "    for i in range(L-tw):\n",
    "        data_seq = input[i:i+tw]\n",
    "        data_label = input[i+tw:i+tw+1][0][3]\n",
    "        inout_seq.append((data_seq ,data_label))\n",
    "    \n",
    "    data = inout_seq;\n",
    "    print('size of data : ', len(data))\n",
    "    test_set_size = 20\n",
    "    train_set_size = len(data) - (test_set_size);\n",
    "    print('size of test : ', test_set_size)\n",
    "    print('size of train : ', train_set_size)\n",
    "    \n",
    "    train = data[:train_set_size]\n",
    "    test = data[train_set_size:]\n",
    "  \n",
    "    return train,test\n",
    "    \n",
    "\n",
    "train,test = create_inout_sequences(price, train_window )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"create_intout_sequences\" method creates labels for the dataset and isolates the datapoints we are inputting into the model, taking into account the training and test size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the first five elements of the train data to confirm its formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data type of the train data is an array, and the data is all normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the LSTM method. The number of inputs included in the model is 4 to represent the Open, High, Low and Close values we are feeding into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must define an LSTM() method object and the loss function and optimizer so that we can use it when we train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the necessary parameters have been defined, we can use the \"LSTM()\" and \"create_inout_sequence()\" to train the model. We are training this model to 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train:\n",
    "        seq = torch.from_numpy(np.array(seq)).type(torch.Tensor)\n",
    "        #print(seq)\n",
    "        labels = torch.from_numpy(np.array(labels)).type(torch.Tensor)\n",
    "        #print(labels)\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "\n",
    "        y_pred = model(seq)\n",
    "        #print('y_pred : ',y_pred)\n",
    "        labels = labels.view(1)\n",
    "        #print('label : ', labels)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method allows us to predict the values using the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "actual=[]\n",
    "pred = []\n",
    "#for i in range(fut_pred):\n",
    "if True: \n",
    "    #seq = test_inout_seq\n",
    "    for seq, labels in test:\n",
    "        seq = torch.from_numpy(np.array(seq)).type(torch.Tensor)\n",
    "        #print(seq)\n",
    "        labels = torch.from_numpy(np.array(labels)).type(torch.Tensor)\n",
    "        actual.append(labels)\n",
    "        with torch.no_grad():\n",
    "            model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "                        torch.zeros(1, 1, model.hidden_layer_size))\n",
    "            pred.append(model(seq).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, our aim is to convert the predicted and actual data into tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.from_numpy(np.array(pred)).type(torch.Tensor)\n",
    "actual = torch.from_numpy(np.array(actual)).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the data to confirm that this it is formatted with the correct data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 individual tensors in both the predicted and actual sets. As we recall, previously, we defined the variable \"test_set_size\" as 20 in the \"create_inout_sequence\" method. These values represent the predicted and actual Close Values for the last (most recent) 20 days.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in order for us to better understand and analyze the accuracy of the model's predictive values, we need to convert the normalized values into thier scalar form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_new = scaler.inverse_transform( np.c_[ np.zeros(20),np.zeros(20),np.zeros(20),np.array(pred)])\n",
    "print(pred_new[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_new = scaler.inverse_transform( np.c_[ np.zeros(20),np.zeros(20),np.zeros(20),np.array(actual)])\n",
    "print(actual_new[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the model did a good job of predicting the Close Value of the stock. We can represent these values visually to see how close they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = plt.figure()\n",
    "    fig2 = plt.figure()\n",
    "    fig3 = plt.figure()\n",
    "    a = fig.add_axes([0,0,1,1])\n",
    "    b = fig2.add_axes([0,0,1,1])\n",
    "    c = fig3.add_axes([0,0,1,1])\n",
    "\n",
    "    a.plot(actual_new[:,3], 'ro')\n",
    "    a.set_ylabel('Stock Value (dollars)')\n",
    "  \n",
    "    b.plot(pred_new[:,3],'c')\n",
    "    b.set_ylabel('Stock Value (dollars)')\n",
    "    #fig.legend(labels = ('Actual','Predicted'),loc='upper left')\n",
    "    \n",
    "    c.plot(actual_new[:,3], 'ro')\n",
    "    c.plot(pred_new[:,3],'c')\n",
    "    c.set_ylabel('Stock Value (dollars)')\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red dots represent the Actual Value of the stock over the last 20 days, and the cayanne line represents the Predicted Value of the stock over the last 20 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the difference between the Actual and Predicted Value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = actual_new - pred_new\n",
    "fig = plt.figure()\n",
    "diffGraph = fig.add_axes([0,0,1,1])\n",
    "diffGraph.plot(difference[:, 3], 'b')\n",
    "diffGraph.set_ylabel('Difference between Actual and Predicted Stock Value (dollars)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully completed creating our RNN model to predict the stock value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We hope this tutorial gave you a glimpse into creating a Recurrent Neural Network using PyTorch. \n",
    "#### Thanks for completing this lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>Want to Learn More?</strong>\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. **PowerAI** speeds up deep learning and AI. Built on IBM’s Power Systems, **PowerAI** is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The **PowerAI** platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IMB Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets. **Watson Studio** is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX). This is the end of this lesson. Thank you for reading this notebook, and good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content and model created by: [Dhivya Lakshminarayanan](https://www.linkedin.com/in/dhivya-lak/), [Samaya Madhavan](https://www.linkedin.com/in/samaya-madhavan)\n",
    "\n",
    "Added to IBM Developer by: [Dhivya Lakshminarayanan](https://www.linkedin.com/in/dhivya-lak/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>References</strong>\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "* https://www.influxdata.com/what-is-time-series-data/\n",
    "* https://ieeexplore.ieee.org/abstract/document/6795963\n",
    "* https://www.youtube.com/watch?v=LHXXI4-IEns\n",
    "* https://www.pluralsight.com/guides/introduction-to-lstm-units-in-rnn\n",
    "* https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5edc29c2ed010d6458d71a83433b383a96a8cbd3efe8531bc90c4b8a5b8bcec9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}