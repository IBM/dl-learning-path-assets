{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"5\">RECURRENT NETWORKS and LSTM IN DEEP LEARNING</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying Recurrent Neural Networks/LSTM for Language Modeling</h2>\n",
    "\n",
    "In this notebook, we will go over the topic of Language Modeling -- a very relevant task that is the cornerstone of many different linguistic problems such as <b>Speech Recognition, Machine Translation and Image Captioning</b>. We will create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and benchmark on the Penn Treebank dataset. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<ol>\n",
    "    <li><a href=\"#language_modeling\">Section 1: What is Language Modeling?</a></li>\n",
    "    <li><a href=\"#treebank_dataset\">Section 2: About the Penn Treebank dataset</a></li>\n",
    "    <li><a href=\"#word_embedding\">Section 3: What are Word Embeddings?</a></li>\n",
    "    <li><a href=\"#building_lstm_model\">Section 4: Building the LSTM model for Language Modeling</a></li>\n",
    "    <li><a href=\"#training_validating_testing\">Section 5: Training, Validating and Testing the model</a></li>\n",
    "</ol>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"language_modeling\"></a>\n",
    "<h2>Section 1: What is Language Modeling?</h2>\n",
    "\n",
    "Language Modeling, to put it simply, <b>is the task of assigning probabilities to sequences of words</b>. Given a context of one or a sequence of words in the language that the language model was trained on, the model should provide the next most probable words or sequence of words that follows from the given sequence of words in the sentence. Language Modeling is one of the most important tasks in Natural Language Processing.\n",
    "\n",
    "<img src=\"https://github.com/IBM/dl-learning-path-assets/raw/main/fundamentals-of-deeplearning/notebooks/images/RNNUsingTF_example_of_sentence_being_predicted.png\" width=\"1080\">\n",
    "<center><i>Example of a sentence being predicted</i></center>\n",
    "<br><br>\n",
    "In this example, one can see the predictions for the next word of the sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- given a word or a sequence of words (the input data) and the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as <b>Speech Recognition, Machine Translation, Image Captioning, Text Correction</b> and many other very relevant problems. \n",
    "\n",
    "<img src=\"https://github.com/IBM/dl-learning-path-assets/raw/main/fundamentals-of-deeplearning/notebooks/images/RNNUsingTF_schema_of_an_rnn_in_execution.png\" width=\"1080\">\n",
    "<center><i>The above example is a schema of an RNN in execution</i></center>\n",
    "<br><br>\n",
    "As the above image shows, Recurrent Neural Network models fit this problem like a glove. Along with LSTM (Long Short-Term Memory) and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem.\n",
    "\n",
    "For Language Modeling problems, <b>perplexity</b> is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that <b>low perplexity means a higher degree of trust in the predictions the model makes</b>. Therefore, the lower perplexity is, the better.\n",
    "\n",
    "The goal for this notebook, therefore, is to create a model that can reach <b>low levels of perplexity</b> on our desired dataset.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"treebank_dataset\"></a>\n",
    "<h2>Section 2: About the Penn Treebank dataset</h2>\n",
    "\n",
    "Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be correct at all. This means that we need a <b>large amount of data, annotated by or at least corrected by humans</b>. This is, of course, not an easy task at all.\n",
    "\n",
    "The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is <i>huge</i> -- there are over <b>four million and eight hundred thousand</b> annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank is commonly used as a benchmark dataset for Language Modeling.\n",
    "\n",
    "The dataset is divided using different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag --<code>&lt;unk&gt;</code>, which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence.\n",
    "<br><br><br>\n",
    "<center>Example of text from the dataset we are going to use, <b>ptb.train</b></center>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <center>the percentage of lung cancer deaths among the workers at the west <code>&lt;unk&gt;</code> mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said the plant which is owned by <code>&lt;unk&gt;</code> & <code>&lt;unk&gt;</code> co. was under contract with <code>&lt;unk&gt;</code> to make the cigarette filters the finding probably will support those who argue that the U.S. should regulate the class of asbestos including <code>&lt;unk&gt;</code> more <code>&lt;unk&gt;</code> than the common kind of asbestos <code>&lt;unk&gt;</code> found in most schools and other buildings dr. <code>&lt;unk&gt;</code> said</center>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"word_embedding\"></a>\n",
    "<h2>Section 3: What are Word Embeddings?</h2>\n",
    "\n",
    "For better processing, in this tutorial, we will make use of <a href=\"https://www.tensorflow.org/tutorials/word2vec/\"><b>word embeddings</b></a>, which is <b>a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers</b>. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be a very effective task in Natural Language Processing, and is a commonplace practice.\n",
    "<br><br>\n",
    "<font size=\"4\"><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30, \\ldots]$$\n",
    "</strong></font>\n",
    "<br>\n",
    "Word Embedding tends to group up similarly used words <i>reasonably</i> close together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and plot these words in a 2-dimensional space, we might see something like this:\n",
    "\n",
    "<img src=\"https://github.com/IBM/dl-learning-path-assets/raw/main/fundamentals-of-deeplearning/notebooks/images/RNNUsingTF_t-sne_mockup_with_clusters_marked_for_easier_visualization.png\" width=\"800\">\n",
    "<center><i>T-SNE Mockup with clusters marked for easier visualization</i></center>\n",
    "<br><br>\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- and they are closer together the higher they are correlated. For example, \"None\" is semantically pretty close to \"Zero\", while a phrase that uses \"Italy\", you could probably also fit \"Germany\" in it, with little damage to the sentence structure. The vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building_lstm_model\"></a>\n",
    "<h2>Section 4: Building the LSTM model for Language Modeling</h2>\n",
    "\n",
    "Now we can start building our model using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Importing and installing modules</h3>\n",
    "    \n",
    "We need to import the necessary modules for our code. We need <b><code>numpy</code></b> and <b><code>tensorflow</code></b>, obviously. Additionally, we can import directly the <b><code>tensorflow.models.rnn</code></b> model, which includes the function for building RNNs, and <b><code>tensorflow.models.rnn.ptb.reader</code></b> which is a helper module for getting the input data from the dataset that we will download and use.\n",
    "\n",
    "If you want to learn more take a look at <https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/scripts/reader.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0rc0\n",
      "  Downloading tensorflow-2.2.0rc0-cp37-cp37m-manylinux2010_x86_64.whl (515.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 515.9 MB 37 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (3.1.0)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (2.1.0)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.1 MB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (3.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (2.1.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (0.34.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (0.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.18.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.27.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow==2.2.0rc0) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (47.3.1.post20200622)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.2.8)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.6.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (0.4.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (19.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (1.5.1)\n",
      "Requirement already satisfied: multidict<5.0,>=4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (4.7.6)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0) (3.7.4.2)\n",
      "Installing collected packages: gast, scipy, astunparse, tensorflow\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.0\n",
      "    Uninstalling scipy-1.5.0:\n",
      "      Successfully uninstalled scipy-1.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.0\n",
      "    Uninstalling tensorflow-2.1.0:\n",
      "      Successfully uninstalled tensorflow-2.1.0\n",
      "Successfully installed astunparse-1.6.3 gast-0.3.3 scipy-1.4.1 tensorflow-2.2.0rc0\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0rc0\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "if not tf.__version__ == '2.2.0-rc0':\n",
    "    print(tf.__version__)\n",
    "    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=RED><h3>IMPORTANT! => If you get an error in the cell above it means your TensorFlow version is not 2.2.0-rc0. Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Output\" and wait until all outputs disappear. Your changes should now be picked up. </h3></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-04 18:29:36--  https://raw.githubusercontent.com/IBM/dl-learning-path-assets/main/fundamentals-of-deeplearning/scripts/reader.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3859 (3.8K) [text/plain]\n",
      "Saving to: ‘reader.py’\n",
      "\n",
      "reader.py           100%[===================>]   3.77K  --.-KB/s    in 0s      \n",
      "\n",
      "2020-11-04 18:29:36 (47.4 MB/s) - ‘reader.py’ saved [3859/3859]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/IBM/dl-learning-path-assets/main/fundamentals-of-deeplearning/scripts/reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Download the dataset</h3>\n",
    "\n",
    "Next, we need to download and extract the <code>simple-examples</code> dataset, which can be done by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-04 18:29:38--  https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz?raw=true\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz [following]\n",
      "--2020-11-04 18:29:38--  https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz [following]\n",
      "--2020-11-04 18:29:38--  https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/octet-stream]\n",
      "Saving to: ‘simple-examples.tgz’\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M   145MB/s    in 0.2s    \n",
      "\n",
      "2020-11-04 18:29:39 (145 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget -O simple-examples.tgz https://github.com/IBM/dl-learning-path-assets/blob/main/fundamentals-of-deeplearning/data/simple-examples.tgz?raw=true\n",
    "!tar xzf simple-examples.tgz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Declaring the models hyperparameters</h3>\n",
    "\n",
    "For the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change them!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size_l1 = 256\n",
    "hidden_size_l2 = 128\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch_decay_lr = 4\n",
    "#The total number of epochs in training\n",
    "max_epoch = 15\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "embeding_vector_size= 200\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"data/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Some clarifications for LSTM architecture based on the arguments:\n",
    "\n",
    "Network structure:\n",
    "\n",
    "<ul>\n",
    "    <li>In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "    </li>\n",
    "    <li>The recurrence steps is set to 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.</li>   \n",
    "    <li>There are 2 hidden layers in the model. Therefore, the structure of the model is like this:\n",
    "        <ul>\n",
    "            <li>200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Input layer: \n",
    "\n",
    "<ul>\n",
    "    <li>The network has 200 input units.</li>\n",
    "    <li>Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "    </li>\n",
    "    <li>The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200].\n",
    "    </li>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "Hidden layer:\n",
    "\n",
    "<ul>\n",
    "    <li>Each LSTM has 200 hidden units which is equivalent to the dimensionality of the embedding words and output.</li>\n",
    "</ul>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to apply what you just learned about language modeling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the <a href=\"https://github.com/tensorflow/models\">PTBModel</a> example bundled with the TensorFlow source code.\n",
    "\n",
    "<h3>Training data</h3>\n",
    "\n",
    "The story starts from the data:\n",
    "<ul>\n",
    "    <li>Train data is a list of words, of size 929589, represented by numbers, e.g. [9971, 9972, 9974, 9975,...]</li>\n",
    "    <li>We read data as mini-batches of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take $$floor(\\frac{N}{b \\times h})+1=1548$$ iterations for the learner to go through all sentences once, where N is the size of the list of words, b is the batch size, and h is the size of each sentence. So, the number of iterators is 1548.\n",
    "    </li>\n",
    "    <li>Each batch data is read from train dataset of size 600, and has a shape of [30x20].</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the data and separate it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, vocab, word_to_id = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929589"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', 'N', '<eos>', 'mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', 'the', 'dutch', 'publishing', 'group', '<eos>', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '<eos>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of']\n"
     ]
    }
   ],
   "source": [
    "def id_to_word(id_list):\n",
    "    line = []\n",
    "    for w in id_list:\n",
    "        for word, wid in word_to_id.items():\n",
    "            if wid == w:\n",
    "                line.append(word)\n",
    "    return line            \n",
    "                \n",
    "\n",
    "print(id_to_word(train_data[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's just read one mini-batch now and feed our network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple = itera.__next__()\n",
    "_input_data = first_touple[0]\n",
    "_targets = first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's look at 3 sentences of our input data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim']\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word(_input_data[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Embeddings</h3>\n",
    "\n",
    "We have to convert the words in our dataset to vectors of numbers. The traditional approach is to use one-hot encoding method that is usually used for converting categorical values to numerical values. However, one-hot encoded vectors are high-dimensional, sparse and in a big dataset, computationally inefficient. So, we use word2vec approach. It is, in fact, a layer in our LSTM network, where the word IDs will be represented as a dense representation before feeding to the LSTM. \n",
    "\n",
    "The embedded vectors also get updated during the training process of the deep neural network.\n",
    "We create the embeddings for our input data. <b>embedding_vocab</b> is a matrix of [10000x200] for all 10000 unique words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>embedding_layer()</b> finds the embedded values for our batch of 30x20 words. It  goes to each row of <code>input_data</code>, and for each word in the row/sentence, finds the corresponding vector in <code>embedding_vocab</code>.<br>\n",
    "It creates a [30x20x200] tensor, so, the first element of <b>inputs</b> (the first sentence), is a matrix of 20x200, and each row of it is a vector representing a word in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embeding_vector_size,batch_input_shape=(batch_size, num_steps),trainable=True,name=\"embedding_vocab\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 200), dtype=float32, numpy=\n",
       "array([[[-0.03945371, -0.04389813, -0.01874912, ..., -0.03403753,\n",
       "          0.01087345,  0.00901997],\n",
       "        [ 0.0206769 ,  0.03475608,  0.02754037, ..., -0.02858514,\n",
       "         -0.01813803, -0.03805057],\n",
       "        [ 0.02005268,  0.0314798 , -0.03039545, ...,  0.02384086,\n",
       "         -0.0341777 ,  0.00046998],\n",
       "        ...,\n",
       "        [ 0.0213947 , -0.02101643, -0.0342048 , ...,  0.00786306,\n",
       "          0.04102099,  0.02725503],\n",
       "        [ 0.00700517, -0.0189576 , -0.01569954, ...,  0.01304462,\n",
       "         -0.00174041,  0.02236081],\n",
       "        [-0.01758896, -0.00061718,  0.03873358, ..., -0.04853742,\n",
       "         -0.00252374,  0.01891393]],\n",
       "\n",
       "       [[-0.00532966,  0.00245203,  0.03643507, ..., -0.02209419,\n",
       "          0.00971473,  0.04543072],\n",
       "        [-0.04985387, -0.01697632,  0.02788527, ..., -0.01146846,\n",
       "         -0.00843192, -0.04110545],\n",
       "        [ 0.04155213, -0.00363164,  0.03933581, ...,  0.01383643,\n",
       "          0.04023489, -0.0204288 ],\n",
       "        ...,\n",
       "        [ 0.02554845, -0.04947772, -0.01003035, ..., -0.02325516,\n",
       "          0.02423662, -0.0268039 ],\n",
       "        [-0.01891191, -0.0442084 , -0.04158493, ...,  0.04433249,\n",
       "          0.00427585, -0.01376188],\n",
       "        [-0.01835902, -0.03474407, -0.01675165, ...,  0.02443006,\n",
       "         -0.03079922, -0.01922953]],\n",
       "\n",
       "       [[ 0.03185317,  0.00352594,  0.0107689 , ..., -0.0294257 ,\n",
       "         -0.0299433 , -0.04824178],\n",
       "        [ 0.03344681, -0.01458086,  0.00966894, ..., -0.04031397,\n",
       "          0.02061638, -0.03739178],\n",
       "        [ 0.01128372, -0.03865667, -0.03608207, ..., -0.04929028,\n",
       "          0.01669027, -0.02001078],\n",
       "        ...,\n",
       "        [-0.02617935,  0.00745736, -0.00606798, ..., -0.0267773 ,\n",
       "         -0.0069553 , -0.04187703],\n",
       "        [ 0.03325098,  0.02333434, -0.00406387, ..., -0.02338264,\n",
       "         -0.00074865,  0.01074224],\n",
       "        [-0.02441133, -0.03964252,  0.01991949, ..., -0.01903806,\n",
       "          0.0032962 ,  0.01749121]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.04828218, -0.03078349, -0.03814327, ...,  0.0206279 ,\n",
       "          0.03003654,  0.04821842],\n",
       "        [ 0.02911821,  0.03885032, -0.03017364, ...,  0.03084339,\n",
       "         -0.04914638,  0.00954168],\n",
       "        [ 0.00401644, -0.00827516,  0.00985156, ...,  0.04715979,\n",
       "         -0.02667642, -0.02579912],\n",
       "        ...,\n",
       "        [-0.02402022, -0.04152377, -0.04023354, ...,  0.00802907,\n",
       "         -0.01543906,  0.04475521],\n",
       "        [ 0.0263244 ,  0.04955515,  0.02260593, ...,  0.01664459,\n",
       "         -0.04332126, -0.04487773],\n",
       "        [-0.04329694,  0.0197869 ,  0.00827563, ..., -0.04237138,\n",
       "          0.04803772, -0.01297512]],\n",
       "\n",
       "       [[-0.00051119,  0.04963908,  0.03724976, ...,  0.01544924,\n",
       "          0.01404419, -0.04409415],\n",
       "        [-0.04474699, -0.0457454 , -0.01691909, ..., -0.01399134,\n",
       "         -0.00509037,  0.04584939],\n",
       "        [ 0.01128372, -0.03865667, -0.03608207, ..., -0.04929028,\n",
       "          0.01669027, -0.02001078],\n",
       "        ...,\n",
       "        [ 0.01680285, -0.02169211,  0.04498816, ...,  0.00800765,\n",
       "          0.01065991, -0.03628336],\n",
       "        [-0.04985387, -0.01697632,  0.02788527, ..., -0.01146846,\n",
       "         -0.00843192, -0.04110545],\n",
       "        [ 0.02064962,  0.00178052,  0.02431268, ..., -0.01563202,\n",
       "          0.00480578,  0.02594188]],\n",
       "\n",
       "       [[ 0.04907192, -0.04979079, -0.01928769, ...,  0.01574019,\n",
       "          0.02164661, -0.00524778],\n",
       "        [-0.00586823, -0.01576088,  0.0055238 , ..., -0.01685783,\n",
       "          0.00175896,  0.04958254],\n",
       "        [-0.04129269, -0.01671533,  0.00841521, ...,  0.03789789,\n",
       "          0.00521912,  0.02140032],\n",
       "        ...,\n",
       "        [ 0.00956764, -0.04650145,  0.04211514, ...,  0.03945487,\n",
       "          0.01444164, -0.01653598],\n",
       "        [-0.00966413,  0.01430936,  0.02374066, ..., -0.04547874,\n",
       "         -0.00842752, -0.04764649],\n",
       "        [-0.04321131, -0.03051361,  0.02632609, ..., -0.00548119,\n",
       "         -0.01865542, -0.04352642]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = embedding_layer(_input_data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Constructing Recurrent Neural Networks</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this step, we create the stacked LSTM using <b>tf.keras.layers.StackedRNNCells</b>, which is a 2 layer LSTM network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>tf.keras.layers.RNN</b> creates a recurrent neural network using <b>stacked_lstm</b>. \n",
    "\n",
    "The input should be a tensor of shape: [batch_size, max_time, embedding_vector_size]; in our case it would be [30, 20, 200].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "<h4>_initial_state</h4>\n",
    "\n",
    "For each LSTM, there are 2 state matrices, c_state and m_state. c_state and m_state represent \"Cell State\" and \"Memory State\". Each hidden layer, has a vector of size 30, which keeps the states. So, for 200 hidden units in each LSTM, we have a matrix of size [30x200].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.inital_state = init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(30, 200) dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.inital_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the outputs. The output of the stackedLSTM comes from 128 hidden_layer, and in each time step(=20), one of them get activated. We use the linear activation to map the 128 hidden layer to a [30X20 matrix].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 20, 128), dtype=float32, numpy=\n",
       "array([[[-8.1811100e-04,  1.7715113e-04,  1.1028619e-03, ...,\n",
       "          4.3937831e-05, -5.8968662e-04,  8.3258434e-05],\n",
       "        [-1.8184722e-03,  2.0966912e-04,  1.4831957e-03, ...,\n",
       "          2.3394006e-03, -3.1051287e-04,  8.8788214e-04],\n",
       "        [-8.6025975e-04,  1.1789273e-03,  5.0066470e-04, ...,\n",
       "          2.8489276e-03, -1.4184717e-03,  8.2448259e-04],\n",
       "        ...,\n",
       "        [-7.5932918e-03, -2.6007921e-03,  7.5018790e-04, ...,\n",
       "          5.4488340e-03, -7.6462668e-03,  3.8094844e-03],\n",
       "        [-9.1757132e-03, -3.1968413e-04, -4.0453466e-05, ...,\n",
       "          5.3883800e-03, -9.0334313e-03,  3.9945184e-03],\n",
       "        [-9.5738983e-03,  4.2160036e-04, -2.8629426e-04, ...,\n",
       "          5.5948151e-03, -8.4349345e-03,  4.6283379e-03]],\n",
       "\n",
       "       [[-9.8285638e-04, -5.3677015e-04, -1.8841519e-04, ...,\n",
       "          3.5656893e-04, -1.7669763e-04, -3.1666891e-04],\n",
       "        [-1.4146112e-03, -1.1330624e-03,  1.0303671e-03, ...,\n",
       "          1.3450067e-04, -1.1066757e-04, -1.8453995e-03],\n",
       "        [-1.0234176e-03, -6.6071883e-04, -2.5363153e-04, ...,\n",
       "         -2.6714806e-05, -1.2541987e-03, -4.8619462e-03],\n",
       "        ...,\n",
       "        [-2.5016034e-03,  1.3637067e-04,  2.9038466e-03, ...,\n",
       "         -3.8709482e-03,  4.8696875e-04, -1.2003082e-04],\n",
       "        [-2.5803109e-03, -1.4197896e-03,  4.3804175e-03, ...,\n",
       "         -4.7867373e-03, -1.9813895e-05,  2.0343428e-04],\n",
       "        [-2.7718132e-03, -1.6498457e-03,  4.5268019e-03, ...,\n",
       "         -4.8198784e-03,  3.5954421e-04,  9.0651144e-04]],\n",
       "\n",
       "       [[-6.1661325e-04,  3.9990337e-04,  4.2498188e-05, ...,\n",
       "         -1.4532600e-04, -4.1219880e-04,  2.2456925e-05],\n",
       "        [ 2.8048095e-04,  1.5060984e-03, -2.4159611e-03, ...,\n",
       "          3.9349968e-04, -1.3960910e-03, -6.0419954e-04],\n",
       "        [ 8.1372575e-04,  2.9049930e-03, -2.9995902e-03, ...,\n",
       "          1.2924074e-03, -1.2608202e-03,  1.1493890e-03],\n",
       "        ...,\n",
       "        [ 3.7877287e-03,  4.6317577e-03,  4.0578917e-03, ...,\n",
       "         -3.5638963e-03, -5.9312535e-03,  1.7937601e-03],\n",
       "        [ 3.6270032e-03,  4.8144720e-03,  3.3513000e-03, ...,\n",
       "         -4.7661169e-03, -4.6547735e-03,  2.1092859e-03],\n",
       "        [ 3.0007127e-03,  5.5871536e-03,  3.4183885e-03, ...,\n",
       "         -5.7179420e-03, -3.2105336e-03,  4.3621981e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-5.6887127e-04,  5.1792129e-04,  3.2873638e-04, ...,\n",
       "          8.4549550e-04,  5.8271881e-04,  9.2597754e-04],\n",
       "        [-2.0684705e-03,  5.3235836e-04, -4.3985152e-04, ...,\n",
       "          2.1607822e-03,  6.3294847e-04, -1.0831741e-03],\n",
       "        [-1.7934034e-03, -4.6734305e-04, -8.6261233e-04, ...,\n",
       "          3.1138828e-03,  9.3301648e-04, -1.6734318e-03],\n",
       "        ...,\n",
       "        [-5.6537380e-04,  3.4962455e-03, -2.7419743e-03, ...,\n",
       "          8.6883560e-04, -1.0222096e-03,  4.8781387e-03],\n",
       "        [-3.3125028e-04,  1.9754535e-03, -3.3147538e-03, ...,\n",
       "          1.1637452e-03, -1.8890497e-03,  5.7621938e-03],\n",
       "        [-1.9268084e-03,  1.1254889e-03, -4.7435267e-03, ...,\n",
       "          2.0847076e-03, -2.4490114e-03,  5.8458978e-03]],\n",
       "\n",
       "       [[ 3.3680610e-06, -2.5732690e-04, -4.0487523e-04, ...,\n",
       "         -3.1341813e-04, -3.6381593e-04, -6.0574769e-04],\n",
       "        [-3.9584818e-04,  6.7286564e-06,  1.9255240e-04, ...,\n",
       "         -3.6757460e-04, -1.7780458e-04, -3.4336820e-03],\n",
       "        [-6.2348804e-04,  9.1314263e-04,  1.2216034e-03, ...,\n",
       "         -2.6042922e-04,  1.1755925e-04, -2.8704209e-03],\n",
       "        ...,\n",
       "        [-3.3992328e-04, -1.7474204e-03,  3.6309161e-03, ...,\n",
       "          5.3169471e-03, -2.0893607e-03, -2.2191494e-03],\n",
       "        [-1.6654208e-03, -1.8070822e-03,  5.4150023e-03, ...,\n",
       "          4.7971350e-03, -2.5230988e-03, -3.8267546e-03],\n",
       "        [-2.2546831e-03, -7.0670946e-04,  6.0533169e-03, ...,\n",
       "          3.5806950e-03, -1.6009113e-03, -5.7053370e-03]],\n",
       "\n",
       "       [[ 7.4033660e-04, -6.5963290e-04, -5.5518834e-04, ...,\n",
       "          1.1177842e-03,  1.1492154e-03,  4.9048528e-04],\n",
       "        [ 2.3149778e-03, -7.0804404e-04, -1.6575801e-03, ...,\n",
       "          3.2374978e-03,  1.7432248e-03,  7.1378401e-04],\n",
       "        [ 2.4762265e-03, -1.0212450e-03, -1.7402272e-03, ...,\n",
       "          4.4684997e-03,  2.9731272e-03,  1.1042116e-03],\n",
       "        ...,\n",
       "        [-3.6066037e-03, -4.4650249e-03, -7.5741275e-04, ...,\n",
       "         -2.5372051e-03, -1.9552838e-03,  1.6429459e-03],\n",
       "        [-3.8791145e-03, -3.9426167e-03, -1.7475970e-03, ...,\n",
       "         -1.0945032e-03, -2.3127887e-03,  2.8724503e-03],\n",
       "        [-3.9386246e-03, -3.3190167e-03, -1.8105523e-03, ...,\n",
       "         -8.7962975e-04, -1.7287416e-03,  4.4755707e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Dense layer</h3>\n",
    "\n",
    "We now create densely-connected neural network layer that would reshape the outputs tensor from  [30 x 20 x 128] to [30 x 20 x 10000].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tf.keras.layers.Dense(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_outputs  = dense(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from dense layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the output from dense layer: \", logits_outputs.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Activation layer</h3>\n",
    "\n",
    "A softmax activation layer is also then applied to derive the probability of the output being in any of the multiclass(10000 in this case) possibilities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.keras.layers.Activation('softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words_prob = activation(logits_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the output from the activation layer:  (30, 20, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of the output from the activation layer: \", output_words_prob.shape) #(batch_size, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the probability of observing words for t=0 to t=20:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of observing words in t=0 to t=20 tf.Tensor(\n",
      "[[1.00025703e-04 1.00031903e-04 1.00017467e-04 ... 1.00006073e-04\n",
      "  9.99896365e-05 9.99800686e-05]\n",
      " [1.00037927e-04 1.00047007e-04 1.00000681e-04 ... 1.00003614e-04\n",
      "  9.99669064e-05 9.99791737e-05]\n",
      " [1.00028738e-04 1.00050514e-04 9.99974000e-05 ... 1.00001904e-04\n",
      "  9.99610129e-05 9.99842741e-05]\n",
      " ...\n",
      " [9.99820768e-05 9.99244076e-05 9.99130280e-05 ... 1.00004509e-04\n",
      "  1.00086334e-04 1.00051533e-04]\n",
      " [1.00003308e-04 9.99286422e-05 9.99333788e-05 ... 1.00021760e-04\n",
      "  1.00070305e-04 1.00067009e-04]\n",
      " [1.00015153e-04 9.99312761e-05 9.99140975e-05 ... 1.00021171e-04\n",
      "  1.00036064e-04 1.00064819e-04]], shape=(20, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"The probability of observing words in t=0 to t=20\", output_words_prob[0,0:num_steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Prediction</h3>\n",
    "\n",
    "What is the word corresponding to the probability output? Let's use the maximum probability:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8225, 2316, 1100, 1196, 8762,  560, 6351, 9080, 1612, 1612, 5196,\n",
       "       1612, 2085, 2085, 7552, 1078, 1078, 1078, 1078, 1078])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output_words_prob[0,0:num_steps], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So, what is the ground truth for the first word of the first sentence? You can get it from the target tensor, if you want to find the embedding vector: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984, 9986,\n",
       "       9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995, 9996], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Objective function</h4>\n",
    "\n",
    "How similar are the predicted words to the target words?\n",
    "\n",
    "Now we have to define our objective function, to calculate the similarity of predicted values to ground truth, and then, penalize the model with the error. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N}\\sum_{i=1}^{N} \\ln p_{\\text{target}\\_i}$$\n",
    "\n",
    "This function is already implemented and available in TensorFlow through _tf.keras.losses.sparse_categorical_crossentropy_. It calculates the categorical cross-entropy loss for <b>logits</b> and the <b>target</b> sequence.  \n",
    "\n",
    "The arguments of this function are:  \n",
    "\n",
    "<ul>\n",
    "    <li>logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].</li>  \n",
    "    <li>targets: List of 1D batch-sized int32 Tensors of the same length as logits.</li>   \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(y_true, y_pred):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss  = crossentropy(_targets, output_words_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's look at the first 10 values of loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([9.210303 , 9.2103815, 9.210497 , 9.210234 , 9.209994 , 9.209065 ,\n",
       "       9.210759 , 9.20978  , 9.210558 , 9.21075  ], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[0,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define cost as average of the losses:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=184.206>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss / batch_size)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Training</h3>\n",
    "\n",
    "To do training for our network, we have to take the following steps:\n",
    "\n",
    "<ol>\n",
    "    <li>Define the optimizer.</li>\n",
    "    <li>Assemble layers to build model.</li>\n",
    "    <li>Calculate the gradients based on the loss function.</li>\n",
    "    <li>Apply the optimizer to the variables/gradients tuple.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>1. Define the optimizer.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "optimizer = tf.keras.optimizers.SGD(lr=lr, clipnorm=max_grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Assemble layers to build model.</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (30, 20, 128)             671088    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layer)\n",
    "model.add(dense)\n",
    "model.add(activation)\n",
    "model.compile(loss=crossentropy, optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>Trainable Variables</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "When defining a variable, if you passed <i>trainable=True</i>, the variable constructor automatically adds new variables to the graph collection <b>GraphKeys.TRAINABLE_VARIABLES</b>. Now, using <i>tf.trainable_variables()</i> you can get all variables created with <b>trainable=True</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can also find the name and scope of all variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding_vocab/embeddings:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell/bias:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/recurrent_kernel:0',\n",
       " 'rnn/stacked_rnn_cells/lstm_cell_1/bias:0',\n",
       " 'dense/kernel:0',\n",
       " 'dense/bias:0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h4>3. Calculate the gradients based on the loss function.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The gradient of a function is the slope of its derivative (line), or in other words, the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and is calculated by the <b>derivative</b> operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's recall the gradient function using an example:\n",
    "$$ z = \\left(2x^2 + 3xy\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(1.0)\n",
    "y =  tf.constant(2.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "    g.watch(x)\n",
    "    g.watch(y)\n",
    "    func_test = 2 * x * x + 3 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>g.gradient()</b> function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors—including variables. <b>g.gradient(func, xs)</b> constructs symbolic partial derivatives of sum of <b>func</b> w.r.t. <i>x</i> in <b>xs</b>. \n",
    "\n",
    "Now, let's look at the derivative w.r.t. <b>var_x</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2 + 3xy\\right) = 4x + 3y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, x) # Will compute to 10.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the derivative w.r.t. <b>var_y</b>:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:y}\\left(2x^2 + 3xy\\right) = 3x $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var_grad = g.gradient(func_test, y) # Will compute to 3.0\n",
    "print(var_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at gradients w.r.t all variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass.\n",
    "    output_words_prob = model(_input_data)\n",
    "    # Loss value for this batch.\n",
    "    loss  = crossentropy(_targets, output_words_prob)\n",
    "    cost = tf.reduce_sum(loss,axis=0) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gradients of loss wrt the trainable variables.\n",
    "grad_t_list = tape.gradient(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.framework.indexed_slices.IndexedSlices object at 0x7f6fafb19790>, <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
      "array([[ 1.0592371e-06,  2.4320696e-07,  1.6768107e-07, ...,\n",
      "         1.4631421e-08, -5.9629525e-07,  1.2437289e-07],\n",
      "       [ 6.9256942e-07, -3.8585455e-09, -8.2669249e-07, ...,\n",
      "        -4.9700841e-08,  3.8061063e-07, -1.8238474e-07],\n",
      "       [-6.3507613e-08,  4.3768125e-08, -7.6234505e-07, ...,\n",
      "        -1.3708402e-07,  4.5389385e-07, -1.1108844e-07],\n",
      "       ...,\n",
      "       [-4.0416527e-08, -5.0678750e-07, -1.3803597e-06, ...,\n",
      "         2.6987382e-07,  4.5803267e-07, -3.6873075e-07],\n",
      "       [-1.9082128e-07,  6.2293992e-07,  3.5736292e-07, ...,\n",
      "        -2.2922966e-07, -1.0020338e-07,  3.8836896e-07],\n",
      "       [ 2.6770414e-07, -4.8688526e-07, -1.7385843e-06, ...,\n",
      "        -6.9703908e-08, -1.7605727e-07,  2.7620285e-07]], dtype=float32)>, <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
      "array([[ 2.1508345e-08,  7.2540075e-08,  2.0223727e-08, ...,\n",
      "         1.5164501e-07,  1.3889917e-07,  5.6566542e-08],\n",
      "       [-8.9117123e-08,  3.3827735e-08,  4.3812857e-08, ...,\n",
      "        -7.1370579e-08,  5.2279887e-09,  4.4307242e-09],\n",
      "       [-1.2039074e-07,  1.4930629e-08,  4.5648623e-09, ...,\n",
      "        -7.2817976e-08,  1.1821605e-08,  1.1879379e-08],\n",
      "       ...,\n",
      "       [ 3.2029725e-08,  2.3099421e-08, -7.7780236e-08, ...,\n",
      "        -1.8436494e-07, -3.7663170e-09, -1.6878078e-08],\n",
      "       [ 6.0820348e-08,  1.4117653e-07, -9.9290602e-08, ...,\n",
      "        -2.8377404e-08, -3.7994050e-07,  3.5637207e-08],\n",
      "       [ 3.9062993e-09,  3.0530142e-08,  7.0731616e-08, ...,\n",
      "        -1.0251671e-07, -5.6594075e-08,  1.8169013e-08]], dtype=float32)>, <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
      "array([ 1.1168749e-05,  6.0362036e-06,  2.8452358e-05, ...,\n",
      "       -8.6891296e-06, -2.7582861e-05, -1.5230588e-05], dtype=float32)>, <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
      "array([[ 1.2494624e-07, -3.2238209e-08,  1.3076159e-07, ...,\n",
      "        -6.6663091e-08, -4.0266895e-07, -3.0506143e-08],\n",
      "       [-1.5124390e-07, -4.8073375e-08, -8.4056836e-08, ...,\n",
      "        -1.4957504e-07, -1.7153567e-10, -1.2519797e-07],\n",
      "       [ 1.4115413e-07,  1.2444453e-07, -3.7728267e-08, ...,\n",
      "        -1.2232326e-07, -9.7569128e-08,  4.0686654e-08],\n",
      "       ...,\n",
      "       [-1.5299555e-08,  2.6334848e-07, -8.0239985e-08, ...,\n",
      "         1.6673192e-07, -4.5650630e-09, -2.4322674e-07],\n",
      "       [-1.4839034e-07, -1.3459638e-07,  6.3266583e-08, ...,\n",
      "        -1.4129152e-07,  1.4337375e-07,  1.8786962e-07],\n",
      "       [ 7.4588584e-08,  1.7329674e-07,  1.6293194e-07, ...,\n",
      "        -1.5321447e-07, -1.9633333e-08, -8.7310767e-08]], dtype=float32)>, <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
      "array([[ 3.74099955e-07,  2.16103842e-07, -9.24928116e-08, ...,\n",
      "         1.16407804e-07, -1.03823552e-07,  1.04961700e-07],\n",
      "       [ 2.40561008e-07,  5.69507499e-08,  1.54115241e-07, ...,\n",
      "        -4.21181312e-10, -1.00962247e-07,  3.21676964e-07],\n",
      "       [ 7.90372425e-08,  1.67820531e-07, -3.34762404e-07, ...,\n",
      "        -1.15599477e-08, -1.26930644e-07, -2.32945951e-07],\n",
      "       ...,\n",
      "       [ 1.16115153e-07,  6.31739780e-08,  1.46478229e-07, ...,\n",
      "        -1.25765382e-07, -1.39845113e-07, -3.95099207e-08],\n",
      "       [-1.84969352e-07, -1.07021748e-07,  1.45937264e-07, ...,\n",
      "        -1.71415593e-07,  4.07896721e-07,  8.81647111e-08],\n",
      "       [ 8.69776358e-08, -4.95544157e-08,  2.08894718e-07, ...,\n",
      "        -1.14927857e-07, -1.46986821e-07,  3.97137427e-07]], dtype=float32)>, <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
      "array([ 5.93007499e-05,  4.41435841e-05, -6.58162799e-06, -6.17705591e-05,\n",
      "        2.14561423e-05, -3.83806546e-05, -3.02651097e-06,  1.71589909e-05,\n",
      "       -3.51840354e-05, -1.66739082e-05, -1.38735923e-05, -7.25714290e-06,\n",
      "        1.31729594e-05, -4.53798930e-06,  1.44039986e-05, -1.28863594e-05,\n",
      "        2.69373013e-05, -2.49957284e-05,  6.33844684e-05,  1.08548647e-06,\n",
      "        1.78042210e-05, -7.96646418e-06, -5.39121811e-06,  2.30585247e-05,\n",
      "       -2.15244654e-07, -9.49056630e-06, -9.23063999e-06, -4.32298621e-05,\n",
      "       -2.54770875e-05,  1.94004297e-05, -2.75877264e-05, -1.72779328e-05,\n",
      "        2.07319663e-05, -3.74865613e-06,  7.65094737e-05,  2.30053010e-05,\n",
      "        5.34611536e-06, -2.97306760e-05, -4.30642140e-05, -3.58683028e-05,\n",
      "        2.70373657e-05, -3.32240234e-05,  4.63837659e-06, -3.02372609e-05,\n",
      "        1.26161467e-07,  7.27798306e-06, -5.15006468e-06, -4.18263153e-06,\n",
      "       -4.34459798e-05, -1.67925500e-05, -4.24852115e-05, -7.27586448e-05,\n",
      "       -3.36644880e-05, -1.03255788e-05, -1.24397382e-04,  2.24576816e-05,\n",
      "        2.98884370e-05,  3.39329636e-05, -1.56340247e-05,  2.66307361e-05,\n",
      "       -5.46380907e-05, -1.52525990e-06, -1.11157151e-05, -3.98660268e-05,\n",
      "       -8.85260761e-07, -3.10645601e-06,  1.56431634e-06, -2.50770099e-05,\n",
      "        1.63807927e-05, -1.51852610e-05, -4.99946782e-06,  1.29953769e-05,\n",
      "       -2.46268905e-06,  2.02767642e-05,  8.99297302e-05, -9.01593830e-06,\n",
      "       -1.30960863e-04, -3.48819549e-06,  1.76051763e-05,  2.66454535e-05,\n",
      "        1.46936363e-05,  2.96639300e-05, -1.20723298e-05,  1.90246301e-05,\n",
      "       -2.67543692e-05, -7.86209057e-05, -2.46025465e-05,  7.62496256e-07,\n",
      "       -3.14693389e-05, -4.37600602e-06,  7.24459460e-06,  1.59403953e-05,\n",
      "       -3.43185311e-05, -1.96765177e-05,  5.44794420e-06, -3.73126677e-05,\n",
      "       -2.98831073e-05, -4.16142166e-06, -4.31098670e-06,  2.83735990e-05,\n",
      "        6.40903636e-07, -6.54821451e-06, -7.62314357e-06, -2.29354227e-05,\n",
      "        5.43844799e-05, -4.44218313e-06, -2.36173182e-05, -5.98441402e-05,\n",
      "        2.98036866e-05,  1.10788096e-05, -1.67456656e-05,  5.79782636e-06,\n",
      "        5.33987532e-07, -1.13279420e-05,  2.77771087e-05,  4.86371982e-06,\n",
      "       -8.66072696e-07,  3.06987204e-06, -1.71386764e-05, -4.31305034e-06,\n",
      "       -2.77980735e-06, -3.09470051e-05,  2.63908114e-05,  1.16106721e-05,\n",
      "       -2.59799272e-05, -4.25067810e-06, -3.22529559e-05,  9.47630724e-06,\n",
      "        4.89548256e-05,  2.81487410e-05, -3.92136462e-06, -8.25501193e-05,\n",
      "        1.80842217e-05, -4.54876063e-05, -2.24905943e-05,  2.84869820e-05,\n",
      "       -5.33308921e-05, -4.96790417e-05, -3.85400563e-06, -4.08881970e-05,\n",
      "        2.35746302e-06,  4.87921170e-06, -1.10457295e-05,  1.24042399e-05,\n",
      "        1.97196205e-05, -6.47303023e-05,  4.45456426e-05, -2.13789608e-05,\n",
      "        1.38626892e-05, -6.84596989e-06,  1.73734988e-05,  3.93233422e-05,\n",
      "        1.97186255e-05, -3.24609464e-05,  9.35181743e-06, -2.89163509e-05,\n",
      "       -2.88905831e-05,  3.75899581e-05, -1.35836053e-05, -2.35079806e-05,\n",
      "        1.27513267e-05, -1.76366393e-05,  1.24065744e-04,  2.17778397e-05,\n",
      "        2.06372024e-05,  3.42518069e-06, -5.94178600e-05, -8.61087901e-05,\n",
      "        4.65618978e-05, -3.10703908e-05,  2.95553218e-05, -2.68552067e-05,\n",
      "       -1.26548421e-05,  1.00100351e-06, -1.86714115e-05,  4.93121297e-06,\n",
      "       -9.45772626e-05, -4.26602201e-05, -4.43496247e-05, -1.00177174e-04,\n",
      "       -2.79446303e-05, -2.83619665e-05, -1.68643775e-04,  4.30178261e-05,\n",
      "        4.20861979e-05,  5.54767539e-05, -3.04359164e-05,  5.83541259e-05,\n",
      "       -3.34085635e-05,  1.10002748e-05, -2.48045071e-05, -4.96503635e-05,\n",
      "       -1.31726802e-06, -2.29249999e-05, -3.54253389e-05, -2.91684082e-05,\n",
      "        1.11568415e-05, -4.82272371e-05, -1.79447270e-05,  1.31134721e-05,\n",
      "        5.35849949e-06,  3.10976757e-05,  1.09838242e-04,  3.70932976e-05,\n",
      "       -2.15458364e-04, -1.51961167e-05,  2.51629463e-05,  1.40479315e-05,\n",
      "        1.58686453e-05,  7.04805207e-05,  8.33450395e-06,  3.65724372e-05,\n",
      "       -3.62051433e-05, -9.99289623e-05, -3.96908217e-05, -1.09180235e-06,\n",
      "       -4.89537124e-05, -3.21425941e-05, -6.73783688e-07,  2.30062415e-06,\n",
      "       -2.73995884e-05, -3.29261784e-05,  3.61673096e-06, -4.16074690e-05,\n",
      "       -2.17760717e-05, -1.21341873e-05, -4.31903391e-06,  2.05090291e-05,\n",
      "       -5.69353142e-06,  5.84391000e-06, -3.95420284e-05, -2.21242190e-05,\n",
      "        7.91659259e-05,  4.04907223e-06, -6.39021673e-05, -6.64502077e-05,\n",
      "        6.19778584e-05,  4.86016143e-06, -7.00548571e-06,  1.50409669e-05,\n",
      "        3.77027550e-06, -8.71238808e-06,  1.42361332e-05, -7.57327734e-06,\n",
      "       -2.42254755e-05,  9.95364280e-06, -3.46272282e-05, -1.61809658e-05,\n",
      "       -4.12388654e-05, -3.91012909e-05,  4.85175260e-05,  1.25210818e-05,\n",
      "       -4.11712381e-06,  2.46406544e-05, -5.15673673e-05,  8.01482565e-06,\n",
      "        5.86543754e-02,  1.18313013e-02, -3.30213085e-02,  2.27446333e-02,\n",
      "       -1.95333194e-02, -1.56639647e-02, -1.90800205e-02, -2.17538215e-02,\n",
      "       -6.68776222e-04,  9.80610261e-04,  2.32889242e-02,  3.81641649e-02,\n",
      "       -2.68124640e-02, -1.39520625e-02,  2.02119574e-02,  1.73478015e-03,\n",
      "       -5.92471771e-02, -2.44758129e-02,  7.93084428e-02,  2.13458259e-02,\n",
      "       -5.21038584e-02, -1.51618093e-03,  8.77135620e-03, -6.39713183e-03,\n",
      "        1.78641863e-02, -6.00593910e-03, -1.39844026e-02,  4.47126552e-02,\n",
      "       -3.73235084e-02, -3.84104787e-03, -6.25116751e-02,  6.47942303e-03,\n",
      "       -1.87098347e-02,  1.29711302e-02,  5.69422841e-02, -3.29563580e-02,\n",
      "       -2.12683100e-02,  1.46161520e-03, -5.63928485e-02, -2.44957637e-02,\n",
      "       -1.12401284e-02,  4.33912314e-03,  2.68972516e-02, -1.44715188e-04,\n",
      "       -2.79694702e-03, -1.70944200e-03,  7.08868541e-03, -1.96992364e-02,\n",
      "       -3.78665477e-02, -1.97674520e-02, -2.15192288e-02, -3.05473469e-02,\n",
      "       -1.46623598e-02, -1.36018731e-02, -4.78228368e-02, -2.96341684e-02,\n",
      "       -1.76612865e-02,  2.10050284e-03,  1.03607904e-02, -2.73049120e-02,\n",
      "        3.83599848e-02, -1.27673289e-02,  7.35498965e-03,  1.81412362e-02,\n",
      "       -1.96291283e-02,  7.63930194e-03,  5.12036774e-03,  6.91664144e-02,\n",
      "        1.24867773e-02,  2.65471055e-03,  1.56542175e-02,  3.01233260e-03,\n",
      "        2.19793972e-02,  5.32244146e-03,  3.60823348e-02,  1.69271640e-02,\n",
      "        6.17497563e-02,  4.71565593e-03,  2.25281715e-03, -1.26190493e-02,\n",
      "        3.61677306e-03,  2.99255382e-02,  6.50016218e-02,  9.78403166e-03,\n",
      "        2.14860663e-02,  4.53009680e-02, -1.69066321e-02, -8.66891537e-03,\n",
      "        7.32571958e-03,  7.64410757e-03,  9.31305066e-03, -5.72529808e-03,\n",
      "        1.42168943e-02, -3.68926451e-02, -4.97640204e-03,  1.76834259e-02,\n",
      "       -1.87431090e-02,  1.01539101e-02,  8.27642716e-03,  1.64745171e-02,\n",
      "       -7.73473829e-03, -1.88201405e-02, -3.93594727e-02,  2.06507985e-02,\n",
      "       -6.92035817e-03, -4.99972999e-02,  1.85774378e-02,  3.12930644e-02,\n",
      "       -2.39293799e-02, -3.54860537e-02, -4.20193234e-03,  1.07302088e-02,\n",
      "       -3.09359394e-02, -3.31955366e-02,  1.65390018e-02, -4.09974717e-02,\n",
      "        2.45246328e-02, -2.81244982e-02, -6.20489568e-03, -3.00406124e-02,\n",
      "        4.71762102e-03, -6.23630034e-03, -8.46823398e-03, -3.78619507e-02,\n",
      "        1.24427043e-02, -1.16967270e-02,  5.08679748e-02,  2.64312383e-02,\n",
      "        6.80646335e-05,  5.09462479e-05, -4.44687021e-06, -5.69601980e-05,\n",
      "        2.47386633e-05, -5.75717386e-05,  1.62711240e-06,  2.41127527e-05,\n",
      "       -4.50594889e-05, -3.16302721e-05, -1.11075933e-05, -7.25635800e-07,\n",
      "        1.10776200e-05,  1.86212310e-06,  1.06253619e-05, -2.02265364e-05,\n",
      "        3.23570639e-05, -2.35198422e-05,  5.30602592e-05, -1.74106572e-06,\n",
      "        1.68070692e-05, -3.95044663e-06,  4.84080829e-06,  2.45476513e-05,\n",
      "        4.80975086e-06, -1.49759744e-05, -1.42972676e-05, -4.41049851e-05,\n",
      "       -3.01933269e-05,  2.17266115e-05, -2.39637902e-05, -2.69970897e-05,\n",
      "        2.59696444e-05, -1.73539120e-05,  1.06532170e-04,  2.26945813e-05,\n",
      "        3.10406085e-06, -2.34644558e-05, -5.14000312e-05, -4.76664973e-05,\n",
      "        3.38813406e-05, -2.84481193e-05, -1.39552503e-06, -3.04828282e-05,\n",
      "        9.11298957e-06,  5.63037975e-06, -1.08193708e-05,  1.36380750e-05,\n",
      "       -5.55149745e-05, -1.41354749e-05, -4.08611013e-05, -8.81037704e-05,\n",
      "       -3.22705382e-05, -1.58053062e-05, -1.39050957e-04,  2.58320506e-05,\n",
      "        3.75750096e-05,  3.47716086e-05, -1.50448523e-05,  4.19282806e-05,\n",
      "       -4.47635175e-05,  6.08731170e-07, -2.52622594e-05, -3.30520124e-05,\n",
      "       -1.47522496e-06, -9.05388879e-06, -3.98727570e-06, -2.19609865e-05,\n",
      "        1.52105749e-05, -2.52484970e-05, -5.99644409e-06,  8.18592889e-06,\n",
      "        1.19391393e-06,  1.48047047e-05,  8.32635124e-05, -2.19050662e-06,\n",
      "       -1.37884999e-04,  1.75404421e-06,  1.97081717e-05,  2.65081326e-05,\n",
      "        1.12699818e-05,  2.21584451e-05, -9.57536031e-07,  3.18584825e-05,\n",
      "       -2.56305611e-05, -9.27308720e-05, -3.22095402e-05,  2.41183898e-07,\n",
      "       -3.55711600e-05, -7.53796257e-06,  9.53926246e-06,  1.38803634e-05,\n",
      "       -3.42523817e-05, -2.30873557e-05,  1.04431529e-05, -4.15938230e-05,\n",
      "       -3.02801400e-05, -3.81860218e-06, -3.65703909e-06,  3.23976674e-05,\n",
      "        8.83373013e-06, -1.37387542e-06, -8.22988113e-06, -2.81717985e-05,\n",
      "        5.54532744e-05, -8.16351530e-07, -2.26494303e-05, -7.45160360e-05,\n",
      "        4.70443556e-05,  1.31309116e-05, -2.03947784e-05,  1.51932545e-05,\n",
      "       -3.12709653e-06, -1.57828745e-05,  3.06266229e-05,  5.67847337e-07,\n",
      "       -3.01033106e-06,  3.26246118e-06, -3.07796545e-05, -6.63248284e-06,\n",
      "       -6.16605757e-06, -3.36451849e-05,  2.97917541e-05,  2.06958721e-05,\n",
      "       -1.91257604e-05, -6.78294964e-06, -4.25786238e-05,  8.03215698e-06],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
      "array([[-6.7073957e-04,  9.0075919e-05, -2.6031032e-03, ...,\n",
      "         4.9356754e-07,  4.9351877e-07,  4.9416303e-07],\n",
      "       [-3.3518780e-04, -9.8999019e-04, -2.1994470e-03, ...,\n",
      "         2.1123390e-06,  2.1150847e-06,  2.1131641e-06],\n",
      "       [-1.6704253e-03, -1.2435894e-03, -8.1105949e-04, ...,\n",
      "         1.9218612e-06,  1.9223789e-06,  1.9231345e-06],\n",
      "       ...,\n",
      "       [-1.8850506e-03, -2.1045979e-03, -2.3511173e-03, ...,\n",
      "         3.7393929e-06,  3.7379994e-06,  3.7393229e-06],\n",
      "       [ 8.7161781e-04,  2.4077857e-03,  1.1207579e-03, ...,\n",
      "        -2.6297184e-06, -2.6301764e-06, -2.6307398e-06],\n",
      "       [-4.5194570e-04,  1.3732130e-04, -1.2852143e-03, ...,\n",
      "         2.0631082e-06,  2.0646489e-06,  2.0642183e-06]], dtype=float32)>, <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
      "array([-0.7979982 , -1.0313314 , -1.0313317 , ...,  0.00199985,\n",
      "        0.00200013,  0.00200048], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(grad_t_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we have a list of tensors, t-list. We can use it to find clipped tensors. <b>clip_by_global_norm</b> clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "<b>clip_by_global_norm</b> get <i>t-list</i> as input and returns 2 things:\n",
    "\n",
    "<ul>\n",
    "    <li>a list of clipped tensors, called <i>list_clipped</i></li> \n",
    "    <li>the global norm (global_norm) of all tensors in t_list</li> \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.framework.indexed_slices.IndexedSlices at 0x7f6fafb2de90>,\n",
       " <tf.Tensor: shape=(200, 1024), dtype=float32, numpy=\n",
       " array([[ 1.0592371e-06,  2.4320696e-07,  1.6768107e-07, ...,\n",
       "          1.4631421e-08, -5.9629525e-07,  1.2437289e-07],\n",
       "        [ 6.9256942e-07, -3.8585455e-09, -8.2669249e-07, ...,\n",
       "         -4.9700841e-08,  3.8061063e-07, -1.8238474e-07],\n",
       "        [-6.3507613e-08,  4.3768125e-08, -7.6234505e-07, ...,\n",
       "         -1.3708402e-07,  4.5389385e-07, -1.1108844e-07],\n",
       "        ...,\n",
       "        [-4.0416527e-08, -5.0678750e-07, -1.3803597e-06, ...,\n",
       "          2.6987382e-07,  4.5803267e-07, -3.6873075e-07],\n",
       "        [-1.9082128e-07,  6.2293992e-07,  3.5736292e-07, ...,\n",
       "         -2.2922966e-07, -1.0020338e-07,  3.8836896e-07],\n",
       "        [ 2.6770414e-07, -4.8688526e-07, -1.7385843e-06, ...,\n",
       "         -6.9703908e-08, -1.7605727e-07,  2.7620285e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 1024), dtype=float32, numpy=\n",
       " array([[ 2.1508345e-08,  7.2540075e-08,  2.0223727e-08, ...,\n",
       "          1.5164501e-07,  1.3889917e-07,  5.6566542e-08],\n",
       "        [-8.9117123e-08,  3.3827735e-08,  4.3812857e-08, ...,\n",
       "         -7.1370579e-08,  5.2279887e-09,  4.4307242e-09],\n",
       "        [-1.2039074e-07,  1.4930629e-08,  4.5648623e-09, ...,\n",
       "         -7.2817976e-08,  1.1821605e-08,  1.1879379e-08],\n",
       "        ...,\n",
       "        [ 3.2029725e-08,  2.3099421e-08, -7.7780236e-08, ...,\n",
       "         -1.8436494e-07, -3.7663170e-09, -1.6878078e-08],\n",
       "        [ 6.0820348e-08,  1.4117653e-07, -9.9290602e-08, ...,\n",
       "         -2.8377404e-08, -3.7994050e-07,  3.5637207e-08],\n",
       "        [ 3.9062993e-09,  3.0530142e-08,  7.0731616e-08, ...,\n",
       "         -1.0251671e-07, -5.6594075e-08,  1.8169013e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1024,), dtype=float32, numpy=\n",
       " array([ 1.1168749e-05,  6.0362036e-06,  2.8452358e-05, ...,\n",
       "        -8.6891296e-06, -2.7582861e-05, -1.5230588e-05], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256, 512), dtype=float32, numpy=\n",
       " array([[ 1.2494624e-07, -3.2238209e-08,  1.3076159e-07, ...,\n",
       "         -6.6663091e-08, -4.0266895e-07, -3.0506143e-08],\n",
       "        [-1.5124390e-07, -4.8073375e-08, -8.4056836e-08, ...,\n",
       "         -1.4957504e-07, -1.7153567e-10, -1.2519797e-07],\n",
       "        [ 1.4115413e-07,  1.2444453e-07, -3.7728267e-08, ...,\n",
       "         -1.2232326e-07, -9.7569128e-08,  4.0686654e-08],\n",
       "        ...,\n",
       "        [-1.5299555e-08,  2.6334848e-07, -8.0239985e-08, ...,\n",
       "          1.6673192e-07, -4.5650630e-09, -2.4322674e-07],\n",
       "        [-1.4839034e-07, -1.3459638e-07,  6.3266583e-08, ...,\n",
       "         -1.4129152e-07,  1.4337375e-07,  1.8786962e-07],\n",
       "        [ 7.4588584e-08,  1.7329674e-07,  1.6293194e-07, ...,\n",
       "         -1.5321447e-07, -1.9633333e-08, -8.7310767e-08]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 512), dtype=float32, numpy=\n",
       " array([[ 3.74099955e-07,  2.16103842e-07, -9.24928116e-08, ...,\n",
       "          1.16407804e-07, -1.03823552e-07,  1.04961700e-07],\n",
       "        [ 2.40561008e-07,  5.69507499e-08,  1.54115241e-07, ...,\n",
       "         -4.21181312e-10, -1.00962247e-07,  3.21676964e-07],\n",
       "        [ 7.90372425e-08,  1.67820531e-07, -3.34762404e-07, ...,\n",
       "         -1.15599477e-08, -1.26930644e-07, -2.32945951e-07],\n",
       "        ...,\n",
       "        [ 1.16115153e-07,  6.31739780e-08,  1.46478229e-07, ...,\n",
       "         -1.25765382e-07, -1.39845113e-07, -3.95099207e-08],\n",
       "        [-1.84969352e-07, -1.07021748e-07,  1.45937264e-07, ...,\n",
       "         -1.71415593e-07,  4.07896721e-07,  8.81647111e-08],\n",
       "        [ 8.69776358e-08, -4.95544157e-08,  2.08894718e-07, ...,\n",
       "         -1.14927857e-07, -1.46986821e-07,  3.97137427e-07]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(512,), dtype=float32, numpy=\n",
       " array([ 5.93007499e-05,  4.41435841e-05, -6.58162799e-06, -6.17705591e-05,\n",
       "         2.14561423e-05, -3.83806546e-05, -3.02651097e-06,  1.71589909e-05,\n",
       "        -3.51840354e-05, -1.66739082e-05, -1.38735923e-05, -7.25714290e-06,\n",
       "         1.31729594e-05, -4.53798930e-06,  1.44039986e-05, -1.28863594e-05,\n",
       "         2.69373013e-05, -2.49957284e-05,  6.33844684e-05,  1.08548647e-06,\n",
       "         1.78042210e-05, -7.96646418e-06, -5.39121811e-06,  2.30585247e-05,\n",
       "        -2.15244654e-07, -9.49056630e-06, -9.23063999e-06, -4.32298621e-05,\n",
       "        -2.54770875e-05,  1.94004297e-05, -2.75877264e-05, -1.72779328e-05,\n",
       "         2.07319663e-05, -3.74865613e-06,  7.65094737e-05,  2.30053010e-05,\n",
       "         5.34611536e-06, -2.97306760e-05, -4.30642140e-05, -3.58683028e-05,\n",
       "         2.70373657e-05, -3.32240234e-05,  4.63837659e-06, -3.02372609e-05,\n",
       "         1.26161467e-07,  7.27798306e-06, -5.15006468e-06, -4.18263153e-06,\n",
       "        -4.34459798e-05, -1.67925500e-05, -4.24852115e-05, -7.27586448e-05,\n",
       "        -3.36644880e-05, -1.03255788e-05, -1.24397382e-04,  2.24576816e-05,\n",
       "         2.98884370e-05,  3.39329636e-05, -1.56340247e-05,  2.66307361e-05,\n",
       "        -5.46380907e-05, -1.52525990e-06, -1.11157151e-05, -3.98660268e-05,\n",
       "        -8.85260761e-07, -3.10645601e-06,  1.56431634e-06, -2.50770099e-05,\n",
       "         1.63807927e-05, -1.51852610e-05, -4.99946782e-06,  1.29953769e-05,\n",
       "        -2.46268905e-06,  2.02767642e-05,  8.99297302e-05, -9.01593830e-06,\n",
       "        -1.30960863e-04, -3.48819549e-06,  1.76051763e-05,  2.66454535e-05,\n",
       "         1.46936363e-05,  2.96639300e-05, -1.20723298e-05,  1.90246301e-05,\n",
       "        -2.67543692e-05, -7.86209057e-05, -2.46025465e-05,  7.62496256e-07,\n",
       "        -3.14693389e-05, -4.37600602e-06,  7.24459460e-06,  1.59403953e-05,\n",
       "        -3.43185311e-05, -1.96765177e-05,  5.44794420e-06, -3.73126677e-05,\n",
       "        -2.98831073e-05, -4.16142166e-06, -4.31098670e-06,  2.83735990e-05,\n",
       "         6.40903636e-07, -6.54821451e-06, -7.62314357e-06, -2.29354227e-05,\n",
       "         5.43844799e-05, -4.44218313e-06, -2.36173182e-05, -5.98441402e-05,\n",
       "         2.98036866e-05,  1.10788096e-05, -1.67456656e-05,  5.79782636e-06,\n",
       "         5.33987532e-07, -1.13279420e-05,  2.77771087e-05,  4.86371982e-06,\n",
       "        -8.66072696e-07,  3.06987204e-06, -1.71386764e-05, -4.31305034e-06,\n",
       "        -2.77980735e-06, -3.09470051e-05,  2.63908114e-05,  1.16106721e-05,\n",
       "        -2.59799272e-05, -4.25067810e-06, -3.22529559e-05,  9.47630724e-06,\n",
       "         4.89548256e-05,  2.81487410e-05, -3.92136462e-06, -8.25501193e-05,\n",
       "         1.80842217e-05, -4.54876063e-05, -2.24905943e-05,  2.84869820e-05,\n",
       "        -5.33308921e-05, -4.96790417e-05, -3.85400563e-06, -4.08881970e-05,\n",
       "         2.35746302e-06,  4.87921170e-06, -1.10457295e-05,  1.24042399e-05,\n",
       "         1.97196205e-05, -6.47303023e-05,  4.45456426e-05, -2.13789608e-05,\n",
       "         1.38626892e-05, -6.84596989e-06,  1.73734988e-05,  3.93233422e-05,\n",
       "         1.97186255e-05, -3.24609464e-05,  9.35181743e-06, -2.89163509e-05,\n",
       "        -2.88905831e-05,  3.75899581e-05, -1.35836053e-05, -2.35079806e-05,\n",
       "         1.27513267e-05, -1.76366393e-05,  1.24065744e-04,  2.17778397e-05,\n",
       "         2.06372024e-05,  3.42518069e-06, -5.94178600e-05, -8.61087901e-05,\n",
       "         4.65618978e-05, -3.10703908e-05,  2.95553218e-05, -2.68552067e-05,\n",
       "        -1.26548421e-05,  1.00100351e-06, -1.86714115e-05,  4.93121297e-06,\n",
       "        -9.45772626e-05, -4.26602201e-05, -4.43496247e-05, -1.00177174e-04,\n",
       "        -2.79446303e-05, -2.83619665e-05, -1.68643775e-04,  4.30178261e-05,\n",
       "         4.20861979e-05,  5.54767539e-05, -3.04359164e-05,  5.83541259e-05,\n",
       "        -3.34085635e-05,  1.10002748e-05, -2.48045071e-05, -4.96503635e-05,\n",
       "        -1.31726802e-06, -2.29249999e-05, -3.54253389e-05, -2.91684082e-05,\n",
       "         1.11568415e-05, -4.82272371e-05, -1.79447270e-05,  1.31134721e-05,\n",
       "         5.35849949e-06,  3.10976757e-05,  1.09838242e-04,  3.70932976e-05,\n",
       "        -2.15458364e-04, -1.51961167e-05,  2.51629463e-05,  1.40479315e-05,\n",
       "         1.58686453e-05,  7.04805207e-05,  8.33450395e-06,  3.65724372e-05,\n",
       "        -3.62051433e-05, -9.99289623e-05, -3.96908217e-05, -1.09180235e-06,\n",
       "        -4.89537124e-05, -3.21425941e-05, -6.73783688e-07,  2.30062415e-06,\n",
       "        -2.73995884e-05, -3.29261784e-05,  3.61673096e-06, -4.16074690e-05,\n",
       "        -2.17760717e-05, -1.21341873e-05, -4.31903391e-06,  2.05090291e-05,\n",
       "        -5.69353142e-06,  5.84391000e-06, -3.95420284e-05, -2.21242190e-05,\n",
       "         7.91659259e-05,  4.04907223e-06, -6.39021673e-05, -6.64502077e-05,\n",
       "         6.19778584e-05,  4.86016143e-06, -7.00548571e-06,  1.50409669e-05,\n",
       "         3.77027550e-06, -8.71238808e-06,  1.42361332e-05, -7.57327734e-06,\n",
       "        -2.42254755e-05,  9.95364280e-06, -3.46272282e-05, -1.61809658e-05,\n",
       "        -4.12388654e-05, -3.91012909e-05,  4.85175260e-05,  1.25210818e-05,\n",
       "        -4.11712381e-06,  2.46406544e-05, -5.15673673e-05,  8.01482565e-06,\n",
       "         5.86543754e-02,  1.18313013e-02, -3.30213085e-02,  2.27446333e-02,\n",
       "        -1.95333194e-02, -1.56639647e-02, -1.90800205e-02, -2.17538215e-02,\n",
       "        -6.68776222e-04,  9.80610261e-04,  2.32889242e-02,  3.81641649e-02,\n",
       "        -2.68124640e-02, -1.39520625e-02,  2.02119574e-02,  1.73478015e-03,\n",
       "        -5.92471771e-02, -2.44758129e-02,  7.93084428e-02,  2.13458259e-02,\n",
       "        -5.21038584e-02, -1.51618093e-03,  8.77135620e-03, -6.39713183e-03,\n",
       "         1.78641863e-02, -6.00593910e-03, -1.39844026e-02,  4.47126552e-02,\n",
       "        -3.73235084e-02, -3.84104787e-03, -6.25116751e-02,  6.47942303e-03,\n",
       "        -1.87098347e-02,  1.29711302e-02,  5.69422841e-02, -3.29563580e-02,\n",
       "        -2.12683100e-02,  1.46161520e-03, -5.63928485e-02, -2.44957637e-02,\n",
       "        -1.12401284e-02,  4.33912314e-03,  2.68972516e-02, -1.44715188e-04,\n",
       "        -2.79694702e-03, -1.70944200e-03,  7.08868541e-03, -1.96992364e-02,\n",
       "        -3.78665477e-02, -1.97674520e-02, -2.15192288e-02, -3.05473469e-02,\n",
       "        -1.46623598e-02, -1.36018731e-02, -4.78228368e-02, -2.96341684e-02,\n",
       "        -1.76612865e-02,  2.10050284e-03,  1.03607904e-02, -2.73049120e-02,\n",
       "         3.83599848e-02, -1.27673289e-02,  7.35498965e-03,  1.81412362e-02,\n",
       "        -1.96291283e-02,  7.63930194e-03,  5.12036774e-03,  6.91664144e-02,\n",
       "         1.24867773e-02,  2.65471055e-03,  1.56542175e-02,  3.01233260e-03,\n",
       "         2.19793972e-02,  5.32244146e-03,  3.60823348e-02,  1.69271640e-02,\n",
       "         6.17497563e-02,  4.71565593e-03,  2.25281715e-03, -1.26190493e-02,\n",
       "         3.61677306e-03,  2.99255382e-02,  6.50016218e-02,  9.78403166e-03,\n",
       "         2.14860663e-02,  4.53009680e-02, -1.69066321e-02, -8.66891537e-03,\n",
       "         7.32571958e-03,  7.64410757e-03,  9.31305066e-03, -5.72529808e-03,\n",
       "         1.42168943e-02, -3.68926451e-02, -4.97640204e-03,  1.76834259e-02,\n",
       "        -1.87431090e-02,  1.01539101e-02,  8.27642716e-03,  1.64745171e-02,\n",
       "        -7.73473829e-03, -1.88201405e-02, -3.93594727e-02,  2.06507985e-02,\n",
       "        -6.92035817e-03, -4.99972999e-02,  1.85774378e-02,  3.12930644e-02,\n",
       "        -2.39293799e-02, -3.54860537e-02, -4.20193234e-03,  1.07302088e-02,\n",
       "        -3.09359394e-02, -3.31955366e-02,  1.65390018e-02, -4.09974717e-02,\n",
       "         2.45246328e-02, -2.81244982e-02, -6.20489568e-03, -3.00406124e-02,\n",
       "         4.71762102e-03, -6.23630034e-03, -8.46823398e-03, -3.78619507e-02,\n",
       "         1.24427043e-02, -1.16967270e-02,  5.08679748e-02,  2.64312383e-02,\n",
       "         6.80646335e-05,  5.09462479e-05, -4.44687021e-06, -5.69601980e-05,\n",
       "         2.47386633e-05, -5.75717386e-05,  1.62711240e-06,  2.41127527e-05,\n",
       "        -4.50594889e-05, -3.16302721e-05, -1.11075933e-05, -7.25635800e-07,\n",
       "         1.10776200e-05,  1.86212310e-06,  1.06253619e-05, -2.02265364e-05,\n",
       "         3.23570639e-05, -2.35198422e-05,  5.30602592e-05, -1.74106572e-06,\n",
       "         1.68070692e-05, -3.95044663e-06,  4.84080829e-06,  2.45476513e-05,\n",
       "         4.80975086e-06, -1.49759744e-05, -1.42972676e-05, -4.41049851e-05,\n",
       "        -3.01933269e-05,  2.17266115e-05, -2.39637902e-05, -2.69970897e-05,\n",
       "         2.59696444e-05, -1.73539120e-05,  1.06532170e-04,  2.26945813e-05,\n",
       "         3.10406085e-06, -2.34644558e-05, -5.14000312e-05, -4.76664973e-05,\n",
       "         3.38813406e-05, -2.84481193e-05, -1.39552503e-06, -3.04828282e-05,\n",
       "         9.11298957e-06,  5.63037975e-06, -1.08193708e-05,  1.36380750e-05,\n",
       "        -5.55149745e-05, -1.41354749e-05, -4.08611013e-05, -8.81037704e-05,\n",
       "        -3.22705382e-05, -1.58053062e-05, -1.39050957e-04,  2.58320506e-05,\n",
       "         3.75750096e-05,  3.47716086e-05, -1.50448523e-05,  4.19282806e-05,\n",
       "        -4.47635175e-05,  6.08731170e-07, -2.52622594e-05, -3.30520124e-05,\n",
       "        -1.47522496e-06, -9.05388879e-06, -3.98727570e-06, -2.19609865e-05,\n",
       "         1.52105749e-05, -2.52484970e-05, -5.99644409e-06,  8.18592889e-06,\n",
       "         1.19391393e-06,  1.48047047e-05,  8.32635124e-05, -2.19050662e-06,\n",
       "        -1.37884999e-04,  1.75404421e-06,  1.97081717e-05,  2.65081326e-05,\n",
       "         1.12699818e-05,  2.21584451e-05, -9.57536031e-07,  3.18584825e-05,\n",
       "        -2.56305611e-05, -9.27308720e-05, -3.22095402e-05,  2.41183898e-07,\n",
       "        -3.55711600e-05, -7.53796257e-06,  9.53926246e-06,  1.38803634e-05,\n",
       "        -3.42523817e-05, -2.30873557e-05,  1.04431529e-05, -4.15938230e-05,\n",
       "        -3.02801400e-05, -3.81860218e-06, -3.65703909e-06,  3.23976674e-05,\n",
       "         8.83373013e-06, -1.37387542e-06, -8.22988113e-06, -2.81717985e-05,\n",
       "         5.54532744e-05, -8.16351530e-07, -2.26494303e-05, -7.45160360e-05,\n",
       "         4.70443556e-05,  1.31309116e-05, -2.03947784e-05,  1.51932545e-05,\n",
       "        -3.12709653e-06, -1.57828745e-05,  3.06266229e-05,  5.67847337e-07,\n",
       "        -3.01033106e-06,  3.26246118e-06, -3.07796545e-05, -6.63248284e-06,\n",
       "        -6.16605757e-06, -3.36451849e-05,  2.97917541e-05,  2.06958721e-05,\n",
       "        -1.91257604e-05, -6.78294964e-06, -4.25786238e-05,  8.03215698e-06],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(128, 10000), dtype=float32, numpy=\n",
       " array([[-6.7073957e-04,  9.0075919e-05, -2.6031032e-03, ...,\n",
       "          4.9356754e-07,  4.9351877e-07,  4.9416303e-07],\n",
       "        [-3.3518780e-04, -9.8999019e-04, -2.1994470e-03, ...,\n",
       "          2.1123390e-06,  2.1150847e-06,  2.1131641e-06],\n",
       "        [-1.6704253e-03, -1.2435894e-03, -8.1105949e-04, ...,\n",
       "          1.9218612e-06,  1.9223789e-06,  1.9231345e-06],\n",
       "        ...,\n",
       "        [-1.8850506e-03, -2.1045979e-03, -2.3511173e-03, ...,\n",
       "          3.7393929e-06,  3.7379994e-06,  3.7393229e-06],\n",
       "        [ 8.7161781e-04,  2.4077857e-03,  1.1207579e-03, ...,\n",
       "         -2.6297184e-06, -2.6301764e-06, -2.6307398e-06],\n",
       "        [-4.5194570e-04,  1.3732130e-04, -1.2852143e-03, ...,\n",
       "          2.0631082e-06,  2.0646489e-06,  2.0642183e-06]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10000,), dtype=float32, numpy=\n",
       " array([-0.7979982 , -1.0313314 , -1.0313317 , ...,  0.00199985,\n",
       "         0.00200013,  0.00200048], dtype=float32)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 4.Apply the optimizer to the variables/gradients tuple. </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a class for the LSTM model</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We learned how the model is built step by step. Now, let's create a Class that represents our model. This class needs a few things:\n",
    "\n",
    "<ul>\n",
    "    <li>We have to create the model in accordance with our defined hyperparameters</li>\n",
    "    <li>We have to create the LSTM cell structure and connect them with our RNN structure</li>\n",
    "    <li>We have to create the word embeddings and point them to the input data</li>\n",
    "    <li>We have to create the input structure for our RNN</li>\n",
    "    <li>We need to create a logistic structure to return the probability of our words</li>\n",
    "    <li>We need to create the loss and cost functions for our optimizer to work, and then create the optimizer</li>\n",
    "    <li>And finally, we need to create a training operation that can be run to actually train our model</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.hidden_size_l1 = hidden_size_l1\n",
    "        self.hidden_size_l2 = hidden_size_l2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeding_vector_size = embeding_vector_size\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = 1.0\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Initializing the model using keras Sequential API  #\n",
    "        ###############################################################################\n",
    "        \n",
    "        self._model = tf.keras.models.Sequential()\n",
    "        \n",
    "        ####################################################################\n",
    "        # Creating the word embeddings layer and adding it to the sequence #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            self._embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embeding_vector_size,batch_input_shape=(self.batch_size, self.num_steps),trainable=True,name=\"embedding_vocab\")  #[10000x200]\n",
    "            self._model.add(self._embedding_layer)\n",
    "            \n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM Cells. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument  of LSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A). \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cell_l1 = tf.keras.layers.LSTMCell(hidden_size_l1)\n",
    "        lstm_cell_l2 = tf.keras.layers.LSTMCell(hidden_size_l2)\n",
    "        \n",
    "\n",
    "        \n",
    "        # By taking in the LSTM cells as parameters, the StackedRNNCells function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of stacked simple cells.\n",
    "        stacked_lstm = tf.keras.layers.StackedRNNCells([lstm_cell_l1, lstm_cell_l2])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ########################################################################################################\n",
    "        # Instantiating our RNN model and setting stateful to True to feed forward the state to the next layer #\n",
    "        ########################################################################################################\n",
    "        \n",
    "        self._RNNlayer  =  tf.keras.layers.RNN(stacked_lstm,[batch_size, num_steps],return_state=False,stateful=True,trainable=True)\n",
    "        \n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = tf.Variable(tf.zeros([batch_size,embeding_vector_size]),trainable=False)\n",
    "        self._RNNlayer.inital_state = self._initial_state\n",
    "    \n",
    "        ############################################\n",
    "        # Adding RNN layer to keras sequential API #\n",
    "        ############################################        \n",
    "        self._model.add(self._RNNlayer)\n",
    "        \n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l1,return_sequences=True,stateful=True))\n",
    "        #self._model.add(tf.keras.layers.LSTM(hidden_size_l2,return_sequences=True))\n",
    "        \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Instantiating a Dense layer that connects the output to the vocab_size  and adding layer to model#\n",
    "        ####################################################################################################\n",
    "        self._dense = tf.keras.layers.Dense(self.vocab_size)\n",
    "        self._model.add(self._dense)\n",
    " \n",
    "        \n",
    "        ####################################################################################################\n",
    "        # Adding softmax activation layer and deriving probability to each class and adding layer to model #\n",
    "        ####################################################################################################\n",
    "        self._activation = tf.keras.layers.Activation('softmax')\n",
    "        self._model.add(self._activation)\n",
    "\n",
    "        ##########################################################\n",
    "        # Instantiating the stochastic gradient decent optimizer #\n",
    "        ########################################################## \n",
    "        self._optimizer = tf.keras.optimizers.SGD(lr=self._lr, clipnorm=max_grad_norm)\n",
    "        \n",
    "        \n",
    "        ##############################################################################\n",
    "        # Compiling and summarizing the model stacked using the keras sequential API #\n",
    "        ##############################################################################\n",
    "        self._model.compile(loss=self.crossentropy, optimizer=self._optimizer)\n",
    "        self._model.summary()\n",
    "\n",
    "\n",
    "    def crossentropy(self,y_true, y_pred):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    def train_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = self._model.trainable_variables\n",
    "        # Define the gradient clipping threshold\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass.\n",
    "            output_words_prob = self._model(_input_data)\n",
    "            # Loss value for this batch.\n",
    "            loss  = self.crossentropy(_targets, output_words_prob)\n",
    "            # average across batch and reduce sum\n",
    "            cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "        # Get gradients of loss wrt the trainable variables.\n",
    "        grad_t_list = tape.gradient(cost, tvars)\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        train_op = self._optimizer.apply_gradients(zip(grads, tvars))\n",
    "        return cost\n",
    "        \n",
    "    def test_batch(self,_input_data,_targets):\n",
    "        #################################################\n",
    "        # Creating the Testing Operation for our Model #\n",
    "        #################################################\n",
    "        output_words_prob = self._model(_input_data)\n",
    "        loss  = self.crossentropy(_targets, output_words_prob)\n",
    "        # average across batch and reduce sum\n",
    "        cost = tf.reduce_sum(loss/ self.batch_size)\n",
    "\n",
    "        return cost\n",
    "    @classmethod\n",
    "    def instance(cls) : \n",
    "        return PTBModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to create the methods to run through time -- that is, the <code>run_epoch</code> method to be run at each epoch and a <code>main</code> script which ties all of this together.\n",
    "\n",
    "What our <code>run_epoch</code> method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################################\n",
    "# run_one_epoch takes as parameters  the model instance, the data to be fed, training or testing mode and verbose info #\n",
    "########################################################################################################################\n",
    "def run_one_epoch(m, data,is_training=True,verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.\n",
    "    iters = 0\n",
    "    \n",
    "    m._model.reset_states()\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    "        \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        #y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "        if is_training : \n",
    "            loss=  m.train_batch(x, y)\n",
    "        else :\n",
    "            loss = m.test_batch(x, y)\n",
    "                                   \n",
    "\n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += loss\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size // 10) == 10:\n",
    "            print(\"Itr %d of %d, perplexity: %.3f speed: %.0f wps\" % (step , epoch_size, np.exp(costs / iters), iters * m.batch_size / (time.time() - start_time)))\n",
    "        \n",
    "\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"training_validating_testing\"></a>\n",
    "<h2>Section 5: Training, Validating and Testing the model</h2>\n",
    "\n",
    "Finally, we can create the <code>main</code> method to tie everything together. The code here reads the data from the directory, using the <code>reader</code> helper module, trains the model and finally evaluates the model on both a validating and a testing subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_vocab (Embedding)  (30, 20, 200)             2000000   \n",
      "_________________________________________________________________\n",
      "rnn_1 (RNN)                  (30, 20, 128)             671088    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (30, 20, 10000)           1290000   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (30, 20, 10000)           0         \n",
      "=================================================================\n",
      "Total params: 3,961,088\n",
      "Trainable params: 3,955,088\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Epoch 1 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 4685.069 speed: 1347 wps\n",
      "Itr 164 of 1549, perplexity: 1089.593 speed: 1433 wps\n",
      "Itr 318 of 1549, perplexity: 843.887 speed: 1508 wps\n",
      "Itr 472 of 1549, perplexity: 699.798 speed: 1501 wps\n",
      "Itr 626 of 1549, perplexity: 597.330 speed: 1474 wps\n",
      "Itr 780 of 1549, perplexity: 531.324 speed: 1459 wps\n",
      "Itr 934 of 1549, perplexity: 479.368 speed: 1450 wps\n",
      "Itr 1088 of 1549, perplexity: 440.160 speed: 1447 wps\n",
      "Itr 1242 of 1549, perplexity: 409.935 speed: 1453 wps\n",
      "Itr 1396 of 1549, perplexity: 381.739 speed: 1461 wps\n",
      "Epoch 1 : Train Perplexity: 359.470\n",
      "Epoch 1 : Valid Perplexity: 212.479\n",
      "Epoch 2 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 236.283 speed: 1542 wps\n",
      "Itr 164 of 1549, perplexity: 210.262 speed: 1537 wps\n",
      "Itr 318 of 1549, perplexity: 201.688 speed: 1536 wps\n",
      "Itr 472 of 1549, perplexity: 193.531 speed: 1542 wps\n",
      "Itr 626 of 1549, perplexity: 184.658 speed: 1526 wps\n",
      "Itr 780 of 1549, perplexity: 181.078 speed: 1504 wps\n",
      "Itr 934 of 1549, perplexity: 177.087 speed: 1489 wps\n",
      "Itr 1088 of 1549, perplexity: 173.639 speed: 1484 wps\n",
      "Itr 1242 of 1549, perplexity: 171.197 speed: 1478 wps\n",
      "Itr 1396 of 1549, perplexity: 167.025 speed: 1476 wps\n",
      "Epoch 2 : Train Perplexity: 164.164\n",
      "Epoch 2 : Valid Perplexity: 160.940\n",
      "Epoch 3 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 160.042 speed: 1422 wps\n",
      "Itr 164 of 1549, perplexity: 146.628 speed: 1436 wps\n",
      "Itr 318 of 1549, perplexity: 142.978 speed: 1437 wps\n",
      "Itr 472 of 1549, perplexity: 138.510 speed: 1436 wps\n",
      "Itr 626 of 1549, perplexity: 133.513 speed: 1440 wps\n",
      "Itr 780 of 1549, perplexity: 132.308 speed: 1442 wps\n",
      "Itr 934 of 1549, perplexity: 130.557 speed: 1439 wps\n",
      "Itr 1088 of 1549, perplexity: 129.095 speed: 1440 wps\n",
      "Itr 1242 of 1549, perplexity: 128.336 speed: 1439 wps\n",
      "Itr 1396 of 1549, perplexity: 126.035 speed: 1440 wps\n",
      "Epoch 3 : Train Perplexity: 124.756\n",
      "Epoch 3 : Valid Perplexity: 144.808\n",
      "Epoch 4 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 129.232 speed: 1683 wps\n",
      "Itr 164 of 1549, perplexity: 119.177 speed: 1635 wps\n",
      "Itr 318 of 1549, perplexity: 117.331 speed: 1548 wps\n",
      "Itr 472 of 1549, perplexity: 114.023 speed: 1510 wps\n",
      "Itr 626 of 1549, perplexity: 110.353 speed: 1495 wps\n",
      "Itr 780 of 1549, perplexity: 109.906 speed: 1486 wps\n",
      "Itr 934 of 1549, perplexity: 108.814 speed: 1480 wps\n",
      "Itr 1088 of 1549, perplexity: 107.965 speed: 1474 wps\n",
      "Itr 1242 of 1549, perplexity: 107.662 speed: 1468 wps\n",
      "Itr 1396 of 1549, perplexity: 105.976 speed: 1465 wps\n",
      "Epoch 4 : Train Perplexity: 105.168\n",
      "Epoch 4 : Valid Perplexity: 136.839\n",
      "Epoch 5 : Learning rate: 1.000\n",
      "Itr 10 of 1549, perplexity: 110.659 speed: 1618 wps\n",
      "Itr 164 of 1549, perplexity: 103.893 speed: 1634 wps\n",
      "Itr 318 of 1549, perplexity: 102.497 speed: 1544 wps\n",
      "Itr 472 of 1549, perplexity: 99.620 speed: 1509 wps\n",
      "Itr 626 of 1549, perplexity: 96.491 speed: 1494 wps\n",
      "Itr 780 of 1549, perplexity: 96.340 speed: 1487 wps\n",
      "Itr 934 of 1549, perplexity: 95.668 speed: 1480 wps\n",
      "Itr 1088 of 1549, perplexity: 95.068 speed: 1476 wps\n",
      "Itr 1242 of 1549, perplexity: 94.952 speed: 1473 wps\n",
      "Itr 1396 of 1549, perplexity: 93.651 speed: 1469 wps\n",
      "Epoch 5 : Train Perplexity: 93.079\n",
      "Epoch 5 : Valid Perplexity: 134.102\n",
      "Epoch 6 : Learning rate: 0.500\n",
      "Itr 10 of 1549, perplexity: 98.564 speed: 1411 wps\n",
      "Itr 164 of 1549, perplexity: 90.554 speed: 1440 wps\n",
      "Itr 318 of 1549, perplexity: 87.922 speed: 1446 wps\n",
      "Itr 472 of 1549, perplexity: 84.642 speed: 1450 wps\n",
      "Itr 626 of 1549, perplexity: 81.103 speed: 1441 wps\n",
      "Itr 780 of 1549, perplexity: 80.278 speed: 1445 wps\n",
      "Itr 934 of 1549, perplexity: 79.107 speed: 1445 wps\n",
      "Itr 1088 of 1549, perplexity: 78.103 speed: 1442 wps\n",
      "Itr 1242 of 1549, perplexity: 77.413 speed: 1441 wps\n",
      "Itr 1396 of 1549, perplexity: 75.776 speed: 1438 wps\n",
      "Epoch 6 : Train Perplexity: 74.728\n",
      "Epoch 6 : Valid Perplexity: 122.458\n",
      "Epoch 7 : Learning rate: 0.250\n",
      "Itr 10 of 1549, perplexity: 82.398 speed: 1482 wps\n",
      "Itr 164 of 1549, perplexity: 77.523 speed: 1431 wps\n",
      "Itr 318 of 1549, perplexity: 75.608 speed: 1461 wps\n",
      "Itr 472 of 1549, perplexity: 72.799 speed: 1446 wps\n",
      "Itr 626 of 1549, perplexity: 69.655 speed: 1439 wps\n",
      "Itr 780 of 1549, perplexity: 68.886 speed: 1435 wps\n",
      "Itr 934 of 1549, perplexity: 67.825 speed: 1434 wps\n",
      "Itr 1088 of 1549, perplexity: 66.839 speed: 1436 wps\n",
      "Itr 1242 of 1549, perplexity: 66.113 speed: 1437 wps\n",
      "Itr 1396 of 1549, perplexity: 64.566 speed: 1439 wps\n",
      "Epoch 7 : Train Perplexity: 63.510\n",
      "Epoch 7 : Valid Perplexity: 120.489\n",
      "Epoch 8 : Learning rate: 0.125\n",
      "Itr 10 of 1549, perplexity: 73.586 speed: 1369 wps\n",
      "Itr 164 of 1549, perplexity: 70.383 speed: 1430 wps\n",
      "Itr 318 of 1549, perplexity: 68.828 speed: 1425 wps\n",
      "Itr 472 of 1549, perplexity: 66.287 speed: 1425 wps\n",
      "Itr 626 of 1549, perplexity: 63.378 speed: 1431 wps\n",
      "Itr 780 of 1549, perplexity: 62.661 speed: 1432 wps\n",
      "Itr 934 of 1549, perplexity: 61.683 speed: 1434 wps\n",
      "Itr 1088 of 1549, perplexity: 60.743 speed: 1432 wps\n",
      "Itr 1242 of 1549, perplexity: 60.030 speed: 1431 wps\n",
      "Itr 1396 of 1549, perplexity: 58.568 speed: 1431 wps\n",
      "Epoch 8 : Train Perplexity: 57.554\n",
      "Epoch 8 : Valid Perplexity: 120.510\n",
      "Epoch 9 : Learning rate: 0.062\n",
      "Itr 10 of 1549, perplexity: 69.339 speed: 1480 wps\n",
      "Itr 164 of 1549, perplexity: 66.690 speed: 1422 wps\n",
      "Itr 318 of 1549, perplexity: 65.308 speed: 1423 wps\n",
      "Itr 472 of 1549, perplexity: 62.904 speed: 1426 wps\n",
      "Itr 626 of 1549, perplexity: 60.108 speed: 1442 wps\n",
      "Itr 780 of 1549, perplexity: 59.405 speed: 1483 wps\n",
      "Itr 934 of 1549, perplexity: 58.489 speed: 1497 wps\n",
      "Itr 1088 of 1549, perplexity: 57.576 speed: 1488 wps\n",
      "Itr 1242 of 1549, perplexity: 56.875 speed: 1481 wps\n",
      "Itr 1396 of 1549, perplexity: 55.459 speed: 1475 wps\n",
      "Epoch 9 : Train Perplexity: 54.466\n",
      "Epoch 9 : Valid Perplexity: 120.675\n",
      "Epoch 10 : Learning rate: 0.031\n",
      "Itr 10 of 1549, perplexity: 67.284 speed: 1429 wps\n",
      "Itr 164 of 1549, perplexity: 64.778 speed: 1429 wps\n",
      "Itr 318 of 1549, perplexity: 63.443 speed: 1432 wps\n",
      "Itr 472 of 1549, perplexity: 61.109 speed: 1431 wps\n",
      "Itr 626 of 1549, perplexity: 58.373 speed: 1428 wps\n",
      "Itr 780 of 1549, perplexity: 57.679 speed: 1428 wps\n",
      "Itr 934 of 1549, perplexity: 56.797 speed: 1429 wps\n",
      "Itr 1088 of 1549, perplexity: 55.903 speed: 1437 wps\n",
      "Itr 1242 of 1549, perplexity: 55.205 speed: 1464 wps\n",
      "Itr 1396 of 1549, perplexity: 53.812 speed: 1487 wps\n",
      "Epoch 10 : Train Perplexity: 52.830\n",
      "Epoch 10 : Valid Perplexity: 120.671\n",
      "Epoch 11 : Learning rate: 0.016\n",
      "Itr 10 of 1549, perplexity: 66.139 speed: 1645 wps\n",
      "Itr 164 of 1549, perplexity: 63.782 speed: 1632 wps\n",
      "Itr 318 of 1549, perplexity: 62.445 speed: 1639 wps\n",
      "Itr 472 of 1549, perplexity: 60.147 speed: 1665 wps\n",
      "Itr 626 of 1549, perplexity: 57.442 speed: 1660 wps\n",
      "Itr 780 of 1549, perplexity: 56.753 speed: 1654 wps\n",
      "Itr 934 of 1549, perplexity: 55.884 speed: 1668 wps\n",
      "Itr 1088 of 1549, perplexity: 54.998 speed: 1675 wps\n",
      "Itr 1242 of 1549, perplexity: 54.299 speed: 1672 wps\n",
      "Itr 1396 of 1549, perplexity: 52.919 speed: 1680 wps\n",
      "Epoch 11 : Train Perplexity: 51.942\n",
      "Epoch 11 : Valid Perplexity: 120.416\n",
      "Epoch 12 : Learning rate: 0.008\n",
      "Itr 10 of 1549, perplexity: 65.380 speed: 1653 wps\n",
      "Itr 164 of 1549, perplexity: 63.216 speed: 1771 wps\n",
      "Itr 318 of 1549, perplexity: 61.899 speed: 1706 wps\n",
      "Itr 472 of 1549, perplexity: 59.628 speed: 1689 wps\n",
      "Itr 626 of 1549, perplexity: 56.942 speed: 1704 wps\n",
      "Itr 780 of 1549, perplexity: 56.254 speed: 1686 wps\n",
      "Itr 934 of 1549, perplexity: 55.388 speed: 1683 wps\n",
      "Itr 1088 of 1549, perplexity: 54.505 speed: 1692 wps\n",
      "Itr 1242 of 1549, perplexity: 53.805 speed: 1703 wps\n",
      "Itr 1396 of 1549, perplexity: 52.431 speed: 1713 wps\n",
      "Epoch 12 : Train Perplexity: 51.458\n",
      "Epoch 12 : Valid Perplexity: 120.053\n",
      "Epoch 13 : Learning rate: 0.004\n",
      "Itr 10 of 1549, perplexity: 64.914 speed: 1651 wps\n",
      "Itr 164 of 1549, perplexity: 62.865 speed: 1692 wps\n",
      "Itr 318 of 1549, perplexity: 61.579 speed: 1687 wps\n",
      "Itr 472 of 1549, perplexity: 59.333 speed: 1638 wps\n",
      "Itr 626 of 1549, perplexity: 56.661 speed: 1578 wps\n",
      "Itr 780 of 1549, perplexity: 55.978 speed: 1543 wps\n",
      "Itr 934 of 1549, perplexity: 55.114 speed: 1519 wps\n",
      "Itr 1088 of 1549, perplexity: 54.233 speed: 1503 wps\n",
      "Itr 1242 of 1549, perplexity: 53.533 speed: 1491 wps\n",
      "Itr 1396 of 1549, perplexity: 52.164 speed: 1494 wps\n",
      "Epoch 13 : Train Perplexity: 51.196\n",
      "Epoch 13 : Valid Perplexity: 119.740\n",
      "Epoch 14 : Learning rate: 0.002\n",
      "Itr 10 of 1549, perplexity: 64.653 speed: 1347 wps\n",
      "Itr 164 of 1549, perplexity: 62.651 speed: 1399 wps\n",
      "Itr 318 of 1549, perplexity: 61.386 speed: 1442 wps\n",
      "Itr 472 of 1549, perplexity: 59.161 speed: 1457 wps\n",
      "Itr 626 of 1549, perplexity: 56.500 speed: 1484 wps\n",
      "Itr 780 of 1549, perplexity: 55.823 speed: 1504 wps\n",
      "Itr 934 of 1549, perplexity: 54.961 speed: 1486 wps\n",
      "Itr 1088 of 1549, perplexity: 54.084 speed: 1475 wps\n",
      "Itr 1242 of 1549, perplexity: 53.385 speed: 1467 wps\n",
      "Itr 1396 of 1549, perplexity: 52.018 speed: 1460 wps\n",
      "Epoch 14 : Train Perplexity: 51.053\n",
      "Epoch 14 : Valid Perplexity: 119.545\n",
      "Epoch 15 : Learning rate: 0.001\n",
      "Itr 10 of 1549, perplexity: 64.520 speed: 1423 wps\n",
      "Itr 164 of 1549, perplexity: 62.532 speed: 1420 wps\n",
      "Itr 318 of 1549, perplexity: 61.276 speed: 1415 wps\n",
      "Itr 472 of 1549, perplexity: 59.064 speed: 1446 wps\n",
      "Itr 626 of 1549, perplexity: 56.410 speed: 1452 wps\n",
      "Itr 780 of 1549, perplexity: 55.738 speed: 1445 wps\n",
      "Itr 934 of 1549, perplexity: 54.878 speed: 1441 wps\n",
      "Itr 1088 of 1549, perplexity: 54.003 speed: 1437 wps\n",
      "Itr 1242 of 1549, perplexity: 53.305 speed: 1435 wps\n",
      "Itr 1396 of 1549, perplexity: 51.941 speed: 1436 wps\n",
      "Epoch 15 : Train Perplexity: 50.978\n",
      "Epoch 15 : Valid Perplexity: 119.447\n",
      "Test Perplexity: 114.498\n",
      "Total time taken to train and test the model %d seconds 9632.890142917633\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "# Instantiates the PTBModel class\n",
    "m=PTBModel.instance()   \n",
    "K = tf.keras.backend \n",
    "for i in range(max_epoch):\n",
    "    # Define the decay for this epoch\n",
    "    lr_decay = decay ** max(i - max_epoch_decay_lr, 0.0)\n",
    "    dcr = learning_rate * lr_decay\n",
    "    m._lr = dcr\n",
    "    K.set_value(m._model.optimizer.learning_rate,m._lr)\n",
    "    print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, m._model.optimizer.learning_rate))\n",
    "    # Run the loop for this epoch in the training mode\n",
    "    train_perplexity = run_one_epoch(m, train_data,is_training=True,verbose=True)\n",
    "    print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "    # Run the loop for this epoch in the validation mode\n",
    "    valid_perplexity = run_one_epoch(m, valid_data,is_training=False,verbose=False)\n",
    "    print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "# Run the loop in the testing mode to see how effective was our training\n",
    "test_perplexity = run_one_epoch(m, test_data,is_training=False,verbose=False)\n",
    "print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total time taken to train and test the model %d seconds\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, <b>lower perplexity means that the model is more certain about its prediction</b>. As such, we can be sure that this model is performing well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "* * *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This is the end of the <b>Applying Recurrent Neural Networks to Text Processing</b> notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. **PowerAI** speeds up deep learning and AI. Built on IBM’s Power Systems, **PowerAI** is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The **PowerAI** platform supports popular machine learning libraries and dependencies including TensorFlow, Caffe, Torch, and Theano. You can use [PowerAI on IBM Cloud](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets. **Watson Studio** is IBM’s leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Notebook created by <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121\">Walter Gomes de Amorim Junior</a>, <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>\n",
    "\n",
    "Updated to TF 2.X by <a href=\"https://linkedin.com/in/romeo-kienzler-089b4557\"> Romeo Kienzler </a>, <a href=\"https://www.linkedin.com/in/samaya-madhavan\"> Samaya Madhavan </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright © 2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
